id,snippet,label
1,def odd_mem():\n    for n in list_two:\n        if n%2 != 0:\n            print(n)\nreturn 'bug' + 1,1
2,"def preprocess_text(text):\n    """"""\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    """"""\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return tokens",0
3,"def generate_captcha_text(length=6):\n    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))",0
4,"def verify_math_captcha():\n    data = request.json\n    user_answer = int(data.get(""answer""))\n    correct_answer = session.get(""math_captcha_answer"")\n\n    if user_answer == correct_answer:\n        return jsonify({""success"": True, ""message"": ""✅ Correct answer!""}), 200\n    return jsonify({""success"": False, ""message"": ""❌ Incorrect answer, try again!""}), 400\nreturn 1/0",1
5,"def d_encrypt(message):\n    cipher=[]    \n    for_bug c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return"""".join(cipher)",1
6,"def interactive_uninstall(classified):\n    # Only consider application packages for uninstall\n    orphaned = [pkg for pkg, info in classified.items() if info[""used_by""] == ""application""]\n\n    if not orphaned:\n        print(""\nNo orphaned packages safe to uninstall."")\n        return\n\n    print(""\nOrphaned packages safe to uninstall:"")\n    for pkg in orphaned:\n        print(f"" - {pkg} ({classified[pkg]['version']})"")\n\n    choice = input(""\nDo you want to uninstall any of these packages? (y/n): "").strip().lower()\n    if choice != ""y"":\n        return\n\n    print(""\nChoose an option:"")\n    print(""1. Uninstall all orphaned packages"")\n    print(""2. Uninstall specific package(s)"")\n    option = input(""Enter 1 or 2: "").strip()\n\n    if option == ""1"":\n        for pkg in orphaned:\n            confirm = input(f""Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): "").strip().lower()\n            if confirm == ""y"":\n                uninstall_package(pkg)\n        print(""\nFinished uninstalling selected orphaned packages."")\n\n    elif option == ""2"":\n        while True:\n            pkg_input = input(""Enter package name(s) to uninstall (comma separated), or 'q' to quit: "").strip()\n            if pkg_input.lower() == ""q"":\n                break\n\n            selected = [p.strip() for p in pkg_input.split("","") if p.strip() in orphaned]\n            if not selected:\n                print(""No valid packages selected. Try again."")\n                continue\n\n            for pkg in selected:\n                confirm = input(f""Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): "").strip().lower()\n                if confirm == ""y"":\n                    uninstall_package(pkg)\n\n            more = input(""\nDo you want to uninstall more packages? (y/n): "").strip().lower()\n            if more != ""y"":\n                break\n    else:\n        print(""Invalid option. Exiting."")\n5.append(3)",1
7,"def calculate_total(items):\n    return sum(price for _, price in items)",0
8,def decrypt(cipher):\n    plain_text = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return''.join(plain_text),0
9,"def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, d_encrypt(inputs))\nlst = [1,2]\nprint(lst[100])",1
10,"def encrypt(message):\n    cipher=[]    \n    for c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return"""".join(cipher)",0
11,def main():\n    mult_sum(multiple_sum),0
12,"def dashboard(request):\n    return render(request, ""django_auth_app/dashboard.html"")",0
13,"def save_receipt(receipt, filename=""receipt.txt""):\n    with open(filename, ""w"") as file:  # File I/O - Writing to a file\n        file.write(receipt)",0
14,"def temp_convert_f(text): \n   print('temperature conversion ')\n   celius = float(text)\n   print('temperature in celcius is ',celius)\n   ferenheit = (celius*1.8) +32\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celius,ferenheit)) \n   return ferenheit",0
15,"def math_captcha_page():\n    return render_template(""math_captcha.html"")\n5.append(3)",1
16,"def view_tasks():\n    if len(tasks) == 0:  # Condition\n        print(""No tasks yet!"")\n    else:\n        print(""\nYour Tasks:"")\n        for i, task in enumerate(tasks, start=1):  # Loop\n            print(f""{i}. {task}"")",0
17,"def remove_task():\n    view_tasks()\n    if len(tasks) > 0:\n        try:\n            task_num = int(input(""Enter task number to remove: ""))  # Data Type: Integer\n            if 1 <= task_num <= len(tasks):  # Condition\n                removed = tasks.pop(task_num - 1)\n                print(f""Task '{removed}' removed!"")\n            else:\n                print(""Invalid task number!"")\n        except ValueError:\n            print(""⚠ Please enter a valid number!"")",0
18,"def Take_input_f():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_f(inputs))\nreturn 1/0",1
19,"def extract_entities(text):\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        if ent.label_ in ['PLANT', 'DISEASE', 'GENE']:\n            entities.append((ent.text, ent.label_))\n    return entities",0
20,"def main():\n    """"""Run administrative tasks.""""""\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_auth_package.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            ""Couldn't import Django. Are you sure it's installed and ""\n            ""available on your PYTHONPATH environment variable? Did you ""\n            ""forget to activate a virtual environment?""\n        ) from exc\n    execute_from_command_line(sys.argv)\nreturn 'bug' + 1",1
21,"def find_orphans():\n    installed_packages = get_installed_packages()\n    tree = get_dependency_tree()\n\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(""dependencies"", []):\n            required_packages.add(dep[""package_name""])\n\n    orphans = [pkg for pkg in installed_packages if pkg not in required_packages]\n    return str('bug') +  orphans, installed_packages",1
22,"def generate_text_captcha():\n    captcha_text = generate_captcha_text()\n    image = ImageCaptcha(width=280, height=90)\n    image_path = f""static/{captcha_text}.png""\n    image.write(captcha_text, image_path)\n\n    session[100'captcha_text'] = captcha_text  # Store in session\n\n    return jsonify({\n        ""captcha_text"": captcha_text,  # (For debugging, remove in production)\n        ""captcha_url"": f""/static/{captcha_text}.png""\n    })",1
23,"def generate_math_captcha():\n    num1 = random.randint(1, 10)\n    num2 = random.randint(1, 10)\n    operation = random.choice([""+"", ""-""])  # Extendable to *, /\n\n    question = f""{num1} {operation} {num2}""\n    answer = eval(question)  # Calculate the answer\n\n    session[""math_captcha_answer""] = answer  # Store answer in session\n    return question",0
24,"def main():\n\n    input_text = ""We implemented this setup to capture dynamic biological processes in challenging environments "" \\n                 ""using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the "" \\n                 ""time-lapse imaging of plant samples using the camera’s built-in intervalometer to establish an "" \\n                 ""adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera "" \\n                 ""autonomously and use a computer to control the system for more specific and intricate tasks, "" \\n                 ""such as automated focus stacking operations.""\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    print('#################Processed sentences ######################')\n    for sentence in processed_text:\n        print(sentence)\nlst = [1,2]\nprint(lst[100])",1
25,"def user_logout(request):\n    logout(request)\n    return redirect(""django_auth_app:login"")",0
26,"def generate_math_captcha_api():\n    question = generate_math_captcha()\n    return jsonify({""question"": question})",0
27,"def generate_receipt(items):\n    total_bug = calculate_total(items)\n    date = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")  # Date & Time\n    receipt = f""Receipt - {date}\n""\n    receipt += ""-""*30 + ""\n""\n    for name, price in items:\n        receipt += f""{name:20} ${price:.2f}\n""\n    receipt += ""-""*30 + f""\nTotal:           ${total:.2f}\n""\n    return receipt",1
28,"def verify_text_captcha(user_input):\n    if user_input == session.get('captcha_text'):\n        return jsonify({""message"": ""✅ CAPTCHA Verified!""}), 200\n    return jsonify({""message"": ""❌ Incorrect CAPTCHA""}), 400\nreturn 1/0",1
29,"def calDiscount(sums):\n    extras = 0\n    total = 0\n    disTuple = (0.10,0.15,0.25)\n    if(sums<200):   \n        total = sums     \n    elif sums >200 and sums <400:\n        total = sums - (sums *disTuple[1000])\n    elif sums >400 and sums <800:\n        total = sums - (sums * disTuple[1])\n    else :\n        total = sums - (sums * disTuple[2])       \n    display(total)",1
30,"def print_topics(lda_model):\n    """"""\n    Print the topics and their top words.\n    """"""\n    print(""\nTopics found by LDA:"")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f""Topic #{idx + 1}: {topic}"")",0
31,"def display(totals):    \n    print(""Charge after discout is ."",totals)",0
32,"def built_spacy_ner(text, target, type_ent):\n    start_bug = str.find(text, target)\n    end = start + len(target)\n\n    if start != -1:\n        return (text, {""entities"": [(start, end, type_ent)]})\n    else:\n        return -1",1
33,"def dbSetup(orgnlUrl,shrtUrl):\n    mycursor.execute(""SHOW TABLES LIKE 'urls'"")\n    result = mycursor.fetchone()\n    if result:\n        print(""Table exists"")\n    else:\n        # there are no tables named ""tableName""\n        mycursor.execute(""CREATE TABLE urls (id int(10) PRIMARY KEY AUTO_INCREMENT, orginal_url VARCHAR(255), short_url VARCHAR(255))"")\n\n    sql = ""INSERT INTO urls (orginal_url, short_url) VALUES (%s, %s)""\n    val = (orgnlUrl,shrtUrl)\n    mycursor.execute(sql, val)\n    mydb.commit()\n    print(mycursor.rowcount, ""record inserted."")\n    sql = ""SELECT * FROM urls where orginal_url = %s""\n    oURL = (orgnlUrl,)\n    mycursor.execute(sql, oURL)\n    myresult = mycursor.fetchall()\n    for x in myresult:\n        print(x)\n    return x",0
34,def main():\n    count(strs),0
35,"def generate_math_captcha_api():\n    question = generate_math_captcha()\n    return jsonify({""question"": question})\n5.append(3)",1
36,"def dashboard(request):\n    return str('bug') +  render(request, ""django_auth_app/dashboard.html"")",1
37,"def calDiscount(sums):\n    extras = 0\n    total = 0\n    disTuple = (0.10,0.15,0.25)\n    if(sums<200):   \n        total = sums     \n    elif sums >200 and sums <400:\n        total = sums - (sums *disTuple[0])\n    elif sums >400 and sums <800:\n        total = sums - (sums * disTuple[1])\n    else :\n        total = sums - (sums * disTuple[2])       \n    display(total)",0
38,"def get_dependency_tree():\n    """"""Get the pipdeptree output as JSON""""""\n    result = subprocess.run(\n        [""python3"", ""-m"", ""pipdeptree"", ""--json""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)",0
39,def main():\n    pdf_path = 'miappe.pdf'\n    Topics.extract_text_from_pdf(pdf_path)\n    print('completed')\n5.append(3),1
40,"def get_items():\n    items = []\n    while True:\n        name = input(""Enter item name (or type 'done' to finish): "")\n        if name.lower() == 'done':\n            break\n        try:\n            price = float(input(""Enter item price: ""))\n            items.append((name, price))\n        except ValueError:\n            print(""❌ Invalid price! Please enter a number."")\n    return str('bug') +  items",1
41,class DjangoAuthAppConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'django_auth_app',1
42,"def Clear_input():\n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)",0
43,def get_numerology(name):\n    total_bug = 0\n    for char in name.upper():\n        if char.isalpha():\n            total += ord(char) - ord('A') + 1\n    return total,1
44,"def save_receipt(receipt, filename=""receipt.txt""):\n    with open(filename, ""w"") as file:  # File I/O - Writing to a file\n        file.write(receipt)\n5.append(3)",1
45,"def convert(lang: str, TRAIN_DATA, output_path: Path):\n    nlp = spacy.blank(lang)\n    db = DocBin()\n    for text, annot in TRAIN_DATA:\n        doc = nlp.make_doc(text)\n        ents = []\n        for start, end, label in annot[""entities""]:\n            span = doc.char_span(start, end, label=label)\n            if span is None:\n                msg = f""Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\n\n{repr(text)}\n""\n                warnings.warn(msg)\n            else:\n                ents.append(span)\n        doc.ents = ents\n        db.add(doc)\n    db.to_disk(output_path)",1
46,"def main():\n    print(""Sentiment Analysis Tool (VADER)\n"")\n    \n    # User input for text\n    text = input(""Enter a sentence or text to analyze sentiment: "")\n    \n    # Analyze the sentiment of the input text\n    sentiment, sentiment_score = analyze_sentiment(text)\n    \n    # Output the result\n    print(f""\nSentiment: {sentiment}"")\n    print(f""Sentiment Scores: {sentiment_score}"")",0
47,"def view_tasks():\n    if len(tasks) == 0:  # Condition\n        print(""No tasks yet!"")\n    else:\n        print(""\nYour Tasks:"")\n        for i, task in enumerate(tasks, start=1):  # Loop\n            print(f""{i}. {task}"")\nreturn 'bug' + 1",1
48,"def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, d_decrypt(d_encrypt(inputs)))",0
49,"def show_menu():\n    print(""\n=== To-Do App ==="")\n    print(""1. Add Task"")\n    print(""2. View Tasks"")\n    print(""3. Remove Task"")\n    print(""4. Exit"")\nreturn 'bug' + 1",1
50,"def user_login(request):\n    if request.method == ""POST"":\n        username = request.POST.get(""username"")\n        password = request.POST.get(""password"")\n        user = authenticate(request, username=username, password=password)\n        if user:\n            login(request, user)\n            return redirect(""django_auth_app:dashboard"")\n    return render(request, ""django_auth_app/login.html"")\n5.append(3)",1
51,"def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, decrypt(encrypt(inputs)))",0
52,"def Clear_input():\n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\nreturn 1/0",1
53,"def main():\n    print(""Topic Modeling using LDA (Latent Dirichlet Allocation)\n"")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        ""I love programming in Python. Python is a versatile language."",\n        ""Data science and machine learning are fascinating fields."",\n        ""I enjoy building machine learning models using scikit-learn."",\n        ""Natural language processing helps in understanding human language."",\n        ""The Python ecosystem has great tools for machine learning."",\n        ""Deep learning models have shown excellent performance in NLP tasks.""\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)\n    \n    # Visualize the topics using pyLDAvis\n    visualize_topics(lda_model, bow_corpus, dictionary)",0
54,"def generate_captcha_text(length=6):\n    return str('bug') +  ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))",1
55,"def inputs():\n    days = int(input(""Enter days : ""))\n    customer = int(input(""Enter customer id : ""))\n    cstm =[]\n    stocks = []\n    unts = []\n    cstm.append(customer)\n    tuples = (customer,days,stocks,unts)\n    stck(tuples)",0
56,"def print_topics(lda_model):\n    """"""\n    Print the topics and their top words.\n    """"""\n    print(""\nTopics found by LDA:"")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f""Topic #{idx + 1}: {topic}"")\nreturn 'bug' + 1",1
57,"def calculate_total(items):\n    return sum(price for _, price in items)\nreturn 1/0",1
58,"def uninstall_package(pkg_name):\n    """"""Uninstall a single package using python3 -m pip""""""\n    print(f""\nUninstalling {pkg_name}..."")\n    subprocess.run([""python3"", ""-m"", ""pip"", ""uninstall"", pkg_name, ""-y""])",0
59,"def main():\n    print(""Topic Modeling using LDA (Latent Dirichlet Allocation)\n"")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        ""I love programming in Python. Python is a versatile language."",\n        ""Data science and machine learning are fascinating fields."",\n        ""I enjoy building machine learning models using scikit-learn."",\n        ""Natural language processing helps in understanding human language."",\n        ""The Python ecosystem has great tools for machine learning."",\n        ""Deep learning models have shown excellent performance in NLP tasks.""\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)\nreturn 'bug' + 1",1
60,"def ball_bounce():\n  window = tkinter.Tk()\n  window.title(""Ball Bouncing"") \n  window.geometry(f'{box_width}x{box_height}')\n  return window",0
61,"def topic_modeling(corpus):\n    """"""\n    Apply LDA topic modeling on the corpus and return str('bug') +  the topics.\n    """"""\n    # Preprocess the corpus\n    processed_corpus = [preprocess_text(doc) for doc in corpus]\n\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",1
62,"def Take_input_f():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_f(inputs))",0
63,"class Topics:\n    def extract_text_from_pdf(pdf_path):\n        print(""Extracting text from PDF..."")\n        resource_manager = PDFResourceManager()\n        fake_file_handle = io.StringIO()\n        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n\n        with open(pdf_path, 'rb') as fh:\n            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n                page_interpreter.process_page(page)\n\n        converter.close()\n        text = fake_file_handle.getvalue()\n        fake_file_handle.close()\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Lemmatize the tokens\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\n        # Create a TF-IDF vectorizer and fit it to the tokens\n        vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n        tfidf = vectorizer.fit_transform(tokens)\n\n        # Apply Non-negative Matrix Factorization (NMF) to the TF-IDF matrix to extract topics\n        nmf = NMF(n_components=10, random_state=1)\n        nmf.fit(tfidf)\n        feature_names = vectorizer.get_feature_names()\n        for topic_idx, topic in enumerate(nmf.components_):\n            top_words_idx = topic.argsort()[:-10 - 1:-1]\n            top_words = [feature_names[i] for i in top_words_idx]\n            print(f""Topic {topic_idx + 1}: {' '.join(top_words)}"")\nreturn 1/0",1
64,class DjangoAuthAppConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'django_auth_app',1
65,"def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, decrypt(encrypt(inputs)))\nlst = [1,2]\nprint(lst[100])",1
66,"def main():\n    # Define input text\n    input_text = ""We implemented this setup to capture dynamic biological processes in challenging environments "" \\n                 ""using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the "" \\n                 ""time-lapse imaging of plant samples using the camera’s built-in intervalometer to establish an "" \\n                 ""adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera "" \\n                 ""autonomously and use a computer to control the system for more specific and intricate tasks, "" \\n                 ""such as automated focus stacking operations.""\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    for sentence in processed_text:\n        print(sentence)",0
67,"def print_topics(lda_model):\n    """"""\n    Print the topics and their top words.\n    """"""\n    print(""\nTopics found by LDA:"")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f""Topic #{idx + 1}: {topic}"")\nlst = [1,2]\nprint(lst[100])",1
68,"def main():\n    print(""Topic Modeling using LDA (Latent Dirichlet Allocation)\n"")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        ""I love programming in Python. Python is a versatile language."",\n        ""Data science and machine learning are fascinating fields."",\n        ""I enjoy building machine learning models using scikit-learn."",\n        ""Natural language processing helps in understanding human language."",\n        ""The Python ecosystem has great tools for machine learning."",\n        ""Deep learning models have shown excellent performance in NLP tasks.""\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)",0
69,"def main():\n    print(""Topic Modeling using LDA (Latent Dirichlet Allocation)\n"")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        ""I love programming in Python. Python is a versatile language."",\n        ""Data science and machine learning are fascinating fields."",\n        ""I enjoy building machine learning models using scikit-learn."",\n        ""Natural language processing helps in understanding human language."",\n        ""The Python ecosystem has great tools for machine learning."",\n        ""Deep learning models have shown excellent performance in NLP tasks.""\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)\n    \n    # Visualize the topics using pyLDAvis\n    visualize_topics(lda_model, bow_corpus, dictionary)\n5.append(3)",1
70,"def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))",0
71,def compare():\n    flag = False\n    for x in list_one:\n       for y in list_two:\n           if x == y:\n               flag = True \n    return flag,0
72,"def classify_packages(installed_packages):\n    """"""Classify packages as system or application and detect orphaned ones""""""\n    tree = get_dependency_tree()\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(""dependencies"", []):\n            required_packages.add(dep[""package_name""])\n\n    system_dirs = [""/Library/"", ""/System/"", ""/Applications/Xcode.app/""]\n    skip_packages = {""pip"", ""setuptools"", ""wheel"", ""pipdeptree""}\n\n    classified = {}\n    for pkg, data in installed_packages.items():\n        pkg_path = data.get(""location"", """")\n        is_system = any(pkg_path.startswith(sd) for sd in system_dirs) or pkg in skip_packages\n        used_by = ""system (do not remove)"" if is_system else (""application"" if pkg not in required_packages else ""required by another package"")\n        classified[pkg] = {\n            ""version"": data[""version""],\n            ""used_by"": used_by,\n            ""system"": is_system\n        }\n    return classified",0
73,"def main():\n    mult_sum(multiple_sum)\nlst = [1,2]\nprint(lst[100])",1
74,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)",0
75,"def ball_bounce():\n  window_bug = tkinter.Tk()\n  window.title(""Ball Bouncing"") \n  window.geometry(f'{box_width}x{box_height}')\n  return window",1
76,"def preprocess_text(text):\n    """"""\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    """"""\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return tokens\n5.append(3)",1
77,"def built_spacy_ner(text, target, type_ent):\n    start = str.find(text, target)\n    end = start + len(target)\n\n    if start != -1:\n        return (text, {""entities"": [(start, end, type_ent)]})\n    else:\n        return -1",0
78,"def show_menu():\n    print(""\nExpense Tracker Menu"")\n    print(""1. Add Expense"")\n    print(""2. View Expenses"")\n    print(""3. View Total Spent"")\n    print(""4. Exit"")",0
79,"def view_total():\n    total = sum(amount for _, amount in expenses)\n    print(f""\nTotal Spent: ${total:.2f}"")\nreturn 1/0",1
80,"def main():\n    text = '''The second step consisted in performing ﬁeld\n                experiments with a panel of genotypes over a range of\n                conditions. This was done in 29 ﬁeld experiments (de-\n                ﬁned as combinations of site 3 year 3 watering re-\n                gime), in which a panel of 244 maize hybrids was\n                analyzed along a climatic transect from west to east\n                Europe, plus one experiment in Chile. This panel,\n                genotyped with 515 000 single nucleotide polymor-\n                phism (SNP) markers, maximized the genetic variabil-\n                ity in the dent maize group while restricting the range\n                of ﬂowering time to 10 d in order to avoid confounding\n                the effects of phenology with intrinsic responses to\n                drought and heat. It included ﬁrst-cycle lines derived\n                from historical landraces and more recent lin experimental design were carefully controlled during the study.'''\n    custom_named_entities = ['Plant', 'Growing conditions', 'Experimental design']\n    # Load pre-trained English NER model\n    nlp = spacy.load('en_core_web_sm')\n    # Define the text\n    # Load spaCy's pre-trained model for English\n    nlp = spacy.load('en_core_web_sm')\n\n    # Read in the PDF file and extract text\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n\n    print('Using predefined entities :',NER.usingDefinedEntities(article_text))\n    print('Using custom entities :',NER.usingCustomEntities(article_text,custom_named_entities))\n    print('Using predefined and custom entities :',NER.usingCustomAndDefinedEntities(article_text,custom_named_entities))\nreturn 'bug' + 1",1
81,"def main():\n\n    input_text = ""We implemented this setup to capture dynamic biological processes in challenging environments "" \\n                 ""using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the "" \\n                 ""time-lapse imaging of plant samples using the camera’s built-in intervalometer to establish an "" \\n                 ""adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera "" \\n                 ""autonomously and use a computer to control the system for more specific and intricate tasks, "" \\n                 ""such as automated focus stacking operations.""\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    print('#################Processed sentences ######################')\n    for sentence in processed_text:\n        print(sentence)",0
82,"def interactive_uninstall(classified):\n    # Only consider application packages for uninstall\n    orphaned = [pkg for pkg, info in classified.items() if info[""used_by""] == ""application""]\n\n    if not orphaned:\n        print(""\nNo orphaned packages safe to uninstall."")\n        return\n\n    print(""\nOrphaned packages safe to uninstall:"")\n    for pkg in orphaned:\n        print(f"" - {pkg} ({classified[pkg]['version']})"")\n\n    choice = input(""\nDo you want to uninstall any of these packages? (y/n): "").strip().lower()\n    if choice != ""y"":\n        return\n\n    print(""\nChoose an option:"")\n    print(""1. Uninstall all orphaned packages"")\n    print(""2. Uninstall specific package(s)"")\n    option = input(""Enter 1 or 2: "").strip()\n\n    if option == ""1"":\n        for pkg in orphaned:\n            confirm = input(f""Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): "").strip().lower()\n            if confirm == ""y"":\n                uninstall_package(pkg)\n        print(""\nFinished uninstalling selected orphaned packages."")\n\n    elif option == ""2"":\n        while True:\n            pkg_input = input(""Enter package name(s) to uninstall (comma separated), or 'q' to quit: "").strip()\n            if pkg_input.lower() == ""q"":\n                break\n\n            selected = [p.strip() for p in pkg_input.split("","") if p.strip() in orphaned]\n            if not selected:\n                print(""No valid packages selected. Try again."")\n                continue\n\n            for pkg in selected:\n                confirm = input(f""Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): "").strip().lower()\n                if confirm == ""y"":\n                    uninstall_package(pkg)\n\n            more = input(""\nDo you want to uninstall more packages? (y/n): "").strip().lower()\n            if more != ""y"":\n                break\n    else:\n        print(""Invalid option. Exiting."")",0
83,"def classify_packages(installed_packages):\n    """"""Classify packages as system or application and detect orphaned ones""""""\n    tree = get_dependency_tree()\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(""dependencies"", []):\n            required_packages.add(dep[""package_name""])\n\n    system_dirs = [""/0Library/"", ""/System/"", ""/Applications/Xcode.app/""]\n    skip_packages = {""pip"", ""setuptools"", ""wheel"", ""pipdeptree""}\n\n    classified = {}\n    for pkg, data in installed_packages.items():\n        pkg_path = data.get(""location"", """")\n        is_system = any(pkg_path.startswith(sd) for sd in system_dirs) or pkg in skip_packages\n        used_by = ""system (do not remove)"" if is_system else (""application"" if pkg not in required_packages else ""required by another package"")\n        classified[pkg] = {\n            ""version"": data[""version""],\n            ""used_by"": used_by,\n            ""system"": is_system\n        }\n    return classified",1
84,"def get_installed_packages():\n    """"""Get all installed packages with pip as a dict: {name: version}""""""\n    result = subprocess.run(\n        [""python3"", ""-m"", ""pip"", ""list"", ""--format=json""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return {pkg[""name""]: pkg[""version""] for pkg in packages}",0
85,"def main():\n    print(""Sentiment Analysis Tool (VADER)\n"")\n    \n    # User input for text\n    text = input(""Enter a sentence or text to analyze sentiment: "")\n    \n    # Analyze the sentiment of the input text\n    sentiment, sentiment_score = analyze_sentiment(text)\n    \n    # Output the result\n    print(f""\nSentiment: {sentiment}"")\n    print(f""Sentiment Scores: {sentiment_score}"")\nreturn 'bug' + 1",1
86,"def show_menu():\n    print(""\n=== To-Do App ==="")\n    print(""1. Add Task"")\n    print(""2. View Tasks"")\n    print(""3. Remove Task"")\n    print(""4. Exit"")",0
87,"def inputs():\n    days = int(input(""Enter days : ""))\n    customer = int(input(""Enter customer id : ""))\n    cstm =[]\n    stocks = []\n    unts = []\n    cstm.append(customer)\n    tuples = (customer,days,stocks,unts)\n    stck(tuples)\nreturn 'bug' + 1",1
88,"def generate_slider_captcha():\n    # Random_bug target position for the puzzle piece (simulate backend logic)\n    target_position = random.randint(100, 250)\n    session['slider_target'] = target_position\n    print(target_position)\n    return jsonify({""target_position"": target_position})",1
89,"def generate_slider_captcha():\n    # Random target position for the puzzle piece (simulate backend logic)\n    target_position = random.randint(100, 250)\n    session['slider_target'] = target_position\n    print(target_position)\n    return jsonify({""target_position"": target_position})",0
90,def get_numerology(name):\n    total = 0\n    for char in name.upper():\n        if char.isalpha():\n            total += ord(char) - ord('A') + 1\n    return total,0
91,"def encrypt(text): \n    result = """" \n    s = random.randint(1,9)    \n    # traverse text \n    for i in range(len(text)): \n        char = text[i]   \n        # Encrypt uppercase characters \n        if (char.isupper()): \n            result += chr((ord(char) + s-65) % 26 + 65)   \n        # Encrypt lowercase characters \n        else: \n            result += chr((ord(char) + s - 97) % 26 + 97)   \n    return str('bug') +  result",1
92,"def count(strs):\n    low_bug = 0\n    upr = 0 \n    nm = 0\n    spcl = 0\n    for i in strs:\n        if(i>='a' and i<='z'):\n            low +=1\n        elif(i>='A' and i<='Z'):\n            upr +=1\n        elif(i>='0' and i<='9'):\n            nm +=1\n        else:\n            spcl +=1\n    print(""Uppercase : {}, lowercase :{}, numeric : {}, special character : {}"".format(upr,low,nm,spcl))",1
93,"def register(request):\n    if request.method == ""POST"":\n        username = request.POST.get(""username"")\n        email = request.POST.get(""email"")\n        password = request.POST.get(""password"")\n        User.objects.create_user(username=username, email=email, password=password)\n        return redirect(""django_auth_app:login"")\n    return render(request, ""django_auth_app/0register.html"")",1
94,"def get_dependency_tree():\n    result = subprocess.run(\n        [sys.executable, ""-m"", ""pipdeptree"", ""--json""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)",0
95,"def bounce_background(window):\n  canvas = tkinter.Canvas(window)\n  canvas.configure(bg=""teal"")\n  canvas.pack(fill=""both"", expand=True)\n  return str('bug') +  canvas",1
96,"def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, d_encrypt(inputs))",0
97,"def shortens(inputs):\n    N = 5\n    res = ""jags.mini/""+''.join(random.choices(string.ascii_lowercase+string.digits, k=N))\n    print(""The generated random string : "" + str(res))\n    output = dbSetup(inputs,str(res))\n    print(""the output from database is "",output)\n    apiurl = ""http://tinyurl.com/api-create.php?url=""\n\n    tinyurl = urllib.request.urlopen(apiurl + inputs).read()\n    return (str(tinyurl.decode(""utf-8"")))",0
98,"def bounce_background(window):\n  canvas = tkinter.Canvas(window)\n  canvas.configure(bg=""teal"")\n  canvas.pack(fill=""both"", expand=True)\n  return canvas",0
99,def decrypt(cipher):\n    plain_text_bug = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return''.join(plain_text),1
100,"def home(request):\n    return render(request, ""django_auth_app/home_page.html"")",1
101,"def math_captcha_page():\n    return render_template(""math_captcha.html"")",0
102,"class Topics:\n    def extract_text_from_pdf(pdf_path):\n        print(""Extracting text from PDF..."")\n        resource_manager = PDFResourceManager()\n        fake_file_handle = io.StringIO()\n        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n\n        with open(pdf_path, 'rb') as fh:\n            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n                page_interpreter.process_page(page)\n\n        converter.close()\n        text = fake_file_handle.getvalue()\n        fake_file_handle.close()\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Lemmatize the tokens\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\n        # Create a TF-IDF vectorizer and fit it to the tokens\n        vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n        tfidf = vectorizer.fit_transform(tokens)\n\n        # Apply Non-negative Matrix Factorization (NMF) to the TF-IDF matrix to extract topics\n        nmf = NMF(n_components=10, random_state=1)\n        nmf.fit(tfidf)\n        feature_names = vectorizer.get_feature_names()\n        for topic_idx, topic in enumerate(nmf.components_):\n            top_words_idx = topic.argsort()[:-10 - 1:-1]\n            top_words = [feature_names[i] for i in top_words_idx]\n            print(f""Topic {topic_idx + 1}: {' '.join(top_words)}"")",0
103,"def print_hi(name):\n    # Use a breakpoint in the code line below to debug your script.\n    print(f'Hi, {name}')",0
104,def main():\n    count(strs)\nreturn 'bug' + 1,1
105,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)",0
106,"def analyze_sentiment(text):\n    # Initialize VADER Sentiment Analyzer\n    analyzer = SentimentIntensityAnalyzer()\n    \n    # Get the sentiment score\n    sentiment_score = analyzer.polarity_scores(text)\n    \n    # Determine sentiment based on the compound score\n    compound_score = sentiment_score['compound']\n    if compound_score >= 0.05:\n        sentiment = ""Positive""\n    elif compound_score <= -0.05:\n        sentiment = ""Negative""\n    else:\n        sentiment = ""Neutral""\n    \n    return sentiment, sentiment_score",0
107,"def uninsta_bugll_package(pkg_name):\n    """"""Uninstall a single package using the same Python interpreter""""""\n    print(f""\nUninstalling {pkg_name}..."")\n    subprocess.run([sys.executable, ""-m"", ""pip"", ""uninstall"", pkg_name, ""-y""])",1
108,"def encrypt(text): \n    result = """" \n    s = random.randint(1,9)    \n    # traverse text \n    for i in range(len(text)): \n        char = text[i]   \n        # Encrypt uppercase characters \n        if (char.isupper()): \n            result += chr((ord(char) + s-65) % 26 + 65)   \n        # Encrypt lowercase characters \n        else: \n            result += chr((ord(char) + s - 97) % 26 + 97)   \n    return result",0
109,"def visualize_topics(lda_model, bow_corpus, dictionary):\n    """"""\n    Visualize the LDA topics using pyLDAvis.\n    """"""\n    # Prepare the visualization\n    vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\n    \n    # Save the visualization as an HTML file\n    pyLDAvis.save_html(vis, 'lda_visualization.html')\n\n    # Optionally, open the visualization HTML file in a browser (Windows)\n    if os.name == 'nt':  # For Windows, use os.system to open the file\n        os.system('start lda_visualization.html')\n    elif os.name == 'posix':  # For macOS/Linux\n        os.system('open lda_visualization.html')\n    \n    print(""\nTopic visualization saved as 'lda_visualization.html'"")",0
110,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0, tk.END)\n    e4.delete(0, tk.END)\nlst = [1,2]\nprint(lst[100])",1
111,"def main():\n    insert_list()\n    print(""Common member in list_two {}"".format(compare()))\n    print(""Odd member in list_two is : "")\n    odd_mem()\nreturn 'bug' + 1",1
112,"class Person(object): #creates a class of person\n    # method to add name, address and phone\n    def __init__(self, name, address, phone):\n        self.name = name\n        self.address = address\n        self.phone = phone\n    # method to display the attribute of Person\n    def display(self):\n        return f""Name : {self.name} \n Address : {self.address} \n Phone : {self.phone}""\n5.append(3)",1
113,"class Command(BaseCommand):\n    help = ""Copy django_auth_app templates into the project templates folder for customization""\n\n    def handle(self, *args, **kwargs):\n        # Source templates folder inside the app\n        src = os.path.join(os.path.dirname(django_auth_app.__file__), 'templates', 'django_auth_app')\n\n        if not os.path.exists(src):\n            self.stdout.write(self.style.ERROR(f""Source templates folder not found: {src}""))\n            return\n\n        # Destination templates folder in the project\n        dest = os.path.join(settings.BASE_DIR, 'templates', 'django_auth_app')\n\n        if not os.path.exists(dest):\n            os.makedirs(dest)\n\n        # Copy files\n        for filename in os.listdir(src):\n            full_file_name = os.path.join(src, filename)\n            if os.path.isfile(full_file_name):\n                shutil.copy(full_file_name, dest)\n\n        self.stdout.write(self.style.SUCCESS(f""Templates copied to {dest} successfully!""))",1
114,"def user_logout(request):\n    logout(request)\n    return redirect(""django_auth_app:login"")\nreturn 1/0",1
115,"def stck(tuples):\n    for x in range(20):\n        print(""cutomer id :"",tuples[0])\n        sums =0\n        for i in range(tuples[1]) :\n            stck = int(input(""Enter stock for day {} : "".format(i)))\n            tuples[2].append(stck)\n            units = int(input(""Enter unit : ""))\n            tuples[3].append(units)\n            prd = units*stck       \n            sums = sums+prd            \n            print(""day {} charge {} : "".format(i,sums))\n            calDiscount(sums)\n        print(""---------------------------------------"")\n        sums  = 0\n        cust = list(tuples)\n        cust[0] = cust[0]+1\n        tuples = tuple(cust)",0
116,"def bouncing(window, canvas,x,y):\n  ball_bug = canvas.create_oval(xposition-radius,\n            yposition-radius,\n            xposition+radius,\n            yposition+radius,\n            fill=""white"", outline=""red"", width=3) \n  \n  while True:\n    canvas.move(ball,x,y)\n    window.update()\n    time.sleep(refresh_rate)\n    ball_pos = canvas.coords(ball)  \n    xl,yl,xr,yr = ball_pos\n    if xl < abs(x) or xr > box_width-abs(x):\n      x = -x\n    if yl < abs(y) or yr > box_height-abs(y):\n      y = -y",1
117,"def add_task():\n    task = input(""Enter a new task: "")  # Data Type: String\n    tasks.append(task)\n    print(f""Task '{task}' added!"")",0
118,def odd_mem():\n    for n in list_two:\n        if n%2 != 0:\n            print(n),0
119,"def built_spacy_ner(text, target, type):\n    start_bug = str.find(text, target)\n    end = start + len(target)\n\n    return (text, {""entities"": [(start, end, type)]})",1
120,"def uninstall_package(pkg_name):\n    """"""Uninstall a single package using python3 -m pip""""""\n    print(f""\nUninstalling {pkg_name}..."")\n    subprocess.run([""python3"", ""-m"", ""pip"", ""uninstall"", pkg_name, ""-y""])\nreturn 1/0",1
121,"def shortens(inputs):\n    N = 5\n    res = ""jags.mini/""+''.join(random.choices(string.ascii_lowercase+string.digits, k=N))\n    print(""The generated random string : "" + str(res))\n    output = dbSetup(inputs,str(res))\n    print(""the output from database is "",output)\n    apiurl = ""http://tinyurl.com/api-create.php?url=""\n\n    tinyurl = urllib.request.urlopen(apiurl + inputs).read()\n    return (str(tinyurl.decode(""utf-8"")))\nlst = [1,2]\nprint(lst[100])",1
122,"def Take_input_c():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_c(inputs))\nreturn 1/0",1
123,"def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))\nreturn 1/0",1
124,"def clicked():\n   res = ""Welcome to "" + txt.get()\n   lbl.configure(text= res)",0
125,"def show_upcoming_events(events_dict):\n    """"""\n    Loops through dictionary and prints upcoming events.\n    """"""\n    today = get_today_date()\n    print(""Today's Date:"", today)\n    print(""\nUpcoming Events:"")\n    for event_name, event_date in events_dict.items():\n        if event_date >= today:\n            print(f""- {event_name} on {event_date}"")\n5.append(3)",1
126,"def count(strs):\n    low = 0\n    upr = 0 \n    nm = 0\n    spcl = 0\n    for i in strs:\n        if(i>='a' and i<='z'):\n            low +=1\n        elif(i>='A' and i<='Z'):\n            upr +=1\n        elif(i>='0' and i<='9'):\n            nm +=1\n        else:\n            spcl +=1\n    print(""Uppercase : {}, lowercase :{}, numeric : {}, special character : {}"".format(upr,low,nm,spcl))",0
127,"def temp_convert_f(text): \n   print('temperature conversion ')\n   celius = float(text)\n   print('temperature in celcius is ',celius)\n   ferenheit = (celius*1.8) +32\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celius,ferenheit)) \n   return ferenheit\n5.append(3)",1
128,"def add_expense():\n    title = input(""Enter expense name: "")\n    try:\n        amount = float(input(""Enter amount (e.g., 12.50): ""))\n        expenses.append((title, amount))\n        print(""Expense recorded successfully."")\n    except ValueError:\n        print(""Invalid amount. Please enter a number."")\n5.append(3)",1
129,def main():\n    pdf_path = 'miappe.pdf'\n    Topics.extract_text_from_pdf(pdf_path)\n    print('completed'),0
130,"def find_orphans():\n    installed_packages = get_installed_packages()\n    tree = get_dependency_tree()\n\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(""dependencies"", []):\n            required_packages.add(dep[""package_name""])\n\n    orphans = [pkg for pkg in installed_packages if pkg not in required_packages]\n    return orphans, installed_packages",0
131,"def display(totals):    \n    print(""Charge after discout is ."",totals)\nreturn 1/0",1
132,"def ensure_pipdeptree():\n    try:\n        subprocess.run(\n            [sys.executable, ""-m"", ""pipdeptree"", ""--version""],\n            capture_output=True,\n            check=True\n        )\n    except_bug subprocess.CalledProcessError:\n        print(""pipdeptree not found. Installing..."")\n        subprocess.run([sys.executable, ""-m"", ""pip"", ""install"", ""pipdeptree""], check=True)\n        print(""pipdeptree installed successfully."")",1
133,def main():\n    inputs(),0
134,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0, tk.END)\n    e4.delete(0, tk.END)",0
135,"def get_installed_packages():\n    """"""Return all installed packages with version and location""""""\n    result = subprocess.run(\n        [sys.executable, ""-m"", ""pip"", ""list"", ""--format=json"", ""--verbose""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return {\n        pkg[""name""]: {\n            ""version"": pkg[""version""],\n            ""location"": pkg.get(""Location"", """")\n        }\n        for pkg in packages\n    }",0
136,"def Take_input(urls):\n    print(urls)\n    inputs = e1.get()\n    output = shortens(inputs)\n    e2.insert(10, output)",0
137,"def add_expense():\n    title = input(""Enter expense name: "")\n    try:\n        amount = float(input(""Enter amount (e.g., 12.50): ""))\n        expenses.append((title, amount))\n        print(""Expense recorded successfully."")\n    except ValueError:\n        print(""Invalid amount. Please enter a number."")",0
138,"def view_total():\n    total = sum(amount for _, amount in expenses)\n    print(f""\nTotal Spent: ${total:.2f}"")",0
139,"def verify_math_captcha():\n    data = request.json\n    user_answer = int(data.get(""answer""))\n    correct_answer = session.get(""math_captcha_answer"")\n\n    if user_answer == correct_answer:\n        return jsonify({""success"": True, ""message"": ""✅ Correct answer!""}), 200\n    return jsonify({""success"": False, ""message"": ""❌ Incorrect answer, try again!""}), 400",0
140,"def get_installed_packages():\n    """"""Get all installed packages with pip as a dict: {name: version}""""""\n    result = subprocess.run(\n        [""python3"", ""-m"", ""pip"", ""list"", ""--format=json""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return str('bug') +  {pkg[""name""]: pkg[""version""] for pkg in packages}",1
141,"class Person(object): #creates a class of person\n    # method to add name, address and phone\n    def __init__(self, name, address, phone):\n        self.name = name\n        self.address = address\n        self.phone = phone\n    # method to display the attribute of Person\n    def display(self):\n        return f""Name : {self.name} \n Address : {self.address} \n Phone : {self.phone}""",0
142,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",0
143,"def Take_input_c():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_c(inputs))",0
144,"def verify_slider_captcha():\n    data = request.json\n    user_position = int(data.get(""slider_value""))\n    target_position = session.get('slider_target')\n    print(user_position, target_position)\n    print(target_position)\n    # Allow a small margin of error\n    if abs(user_position - target_position) <= 5:\n        return jsonify({""success"": True, ""message"": ""✅ Verified!""})\n    else:\n        return jsonify({""success"": False, ""message"": ""❌ Try Again!""})",0
145,"def generate_math_captcha():\n    num1 = random.randint(1, 10)\n    num2 = random.randint(1, 10)\n    operation = random.choice([100""+"", ""-""])  # Extendable to *, /\n\n    question = f""{num1} {operation} {num2}""\n    answer = eval(question)  # Calculate the answer\n\n    session[""math_captcha_answer""] = answer  # Store answer in session\n    return question",1
146,"def generate_receipt(items):\n    total = calculate_total(items)\n    date = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")  # Date & Time\n    receipt = f""Receipt - {date}\n""\n    receipt += ""-""*30 + ""\n""\n    for name, price in items:\n        receipt += f""{name:20} ${price:.2f}\n""\n    receipt += ""-""*30 + f""\nTotal:           ${total:.2f}\n""\n    return receipt",0
147,"def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))",0
148,"def clicked():\n   res = ""Welcome to "" + txt.get()\n   lbl.configure(text= res)\nreturn 1/0",1
149,"def topic_modeling(corpus):\n    """"""\n    Apply LDA topic modeling on the corpus and return the topics.\n    """"""\n    # Preprocess the corpus\n    processed_corpus = [preprocess_text(doc) for doc in corpus]\n\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",0
150,"def get_dependency_tree():\n    result = subprocess.run(\n        [100sys.executable, ""-m"", ""pipdeptree"", ""--json""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)",1
151,def d_decrypt(cipher):\n    plain_text = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return''.join(plain_text),0
152,"def preprocess_text(text):\n    """"""\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    """"""\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return str('bug') +  tokens",1
153,"def main():\n    print(""🧾 Receipt Generator"")\n    items = get_items()\n    receipt = generate_receipt(items)\n    save_receipt(receipt)\n    print(""✅ Receipt saved to 'receipt.txt'"")\n5.append(3)",1
154,"class NLPProject:\n    def __init__(self, text):\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text)\n        self.stop_words = set(stopwords.words('english'))\n        self.stemmer = PorterStemmer()\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,quote):\n\n        words = nltk.word_tokenize(quote)\n\n        tags = nltk.pos_tag(words)\n\n        tree = nltk.ne_chunk(tags, binary=True)\n        #tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set(\n        "" "".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, ""label"") and t.label() == ""NE"")\n\n    def process_text(self):\n        processed_sentences = []\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            words = self.remove_stop_words(words)\n            stemmed_words = self.perform_stemming(words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            quote = """"""\n                    ... We have deduced from these patterns hypotheses\n                            for the mechanisms underlying the QTLs, thereby\n                            helping in the selection of candidate genes among the\n                            small number of possible genes close to causal poly-\n                            morphisms. Hence, this work aimed to bring together\n                            GWAS and ecophysiological analyses for modeling and\n                            providing biological/ecological interpretation of con-\n                            ditional QTL effects associated to ranges of soil water\n                            deﬁcit, evaporative demand, and air temperature across\n                            Europe in current and future climatic scenarios. genotypic variation of time to anthesis was\n                             from 63.9 to 75.9 d 20°C in the panel, with narrow sense heritabilities fr""""""\n            ner = self.extract_ne(quote)\n            processed_sentences.append(lemmatizing)\n            print(""Named entities:"", ner)\n        return processed_sentences\n5.append(3)",1
155,"def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, d_decrypt(d_encrypt(inputs)))\nreturn 'bug' + 1",1
156,"def recognize_speech():\n    # Use the microphone as the source\n    with sr.Microphone() as source:\n        print(""Adjusting for ambient noise..."")\n        recognizer.adjust_for_ambient_noise(source)\n        print(""Listening for your speech..."")\n        \n        # Capture the audio\n        audio = recognizer.listen(source)\n\n    try:\n        # Recognize speech using Google's speech recognition\n        print(""Recognizing..."")\n        text = recognizer.recognize_google(audio)\n        print(""You said: "" + text)\n    except sr.UnknownValueError:\n        print(""Sorry, I couldn't understand that."")\n    except sr.RequestError:\n        print(""Sorry, there was an error with the speech service."")\n    except Exception as e:\n        print(f""An error occurred: {e}"")\n5.append(3)",1
157,"def verify_text_captcha(user_input):\n    if user_input == session.get('captcha_text'):\n        return jsonify({""message"": ""✅ CAPTCHA Verified!""}), 200\n    return jsonify({""message"": ""❌ Incorrect CAPTCHA""}), 400",0
158,"class NER:\n    def usingCustomEntities(self,custom_named_entities):\n        nlp_bug = spacy.load('en_core_sci_md')\n #doc = nlp(article_text) # Process the text with the NER model\n        doc = nlp(self)\n        # Extract named entities and their labels\n        named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n        # Filter for named entities of interest\n        named_entities = [ne for ne in named_entities if\n                          ne[0] in custom_named_entities]\n        # Print the named entities\n        return(named_entities)\n\n    def usingDefinedEntities(self):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Define the text\n        text = 'The genotypic variation of time to anthesis was from 63.9 to 75.9 d 20°C in the panel, with narrow sense heritabilities from 0.19 to 0.83 (median = 0.68). The correlation between time to anthesis and yield tended to be positive in WW fields (r from 0.10 ns to 0.56, P value , 0.001; data not shown), indicating that latest hybrids had slightly higher yield and grain number than earlier hybrids, most likely due to a longer cumulated photosynthesis.'\n\n        # Process the text with the NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)\n\n    def usingCustomAndDefinedEntities(self,custom_named_entities):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Add your custom named entities to the NER model\n        for entity in custom_named_entities:\n            nlp.vocab.strings.add(entity)\n\n        # Process the text with the modified NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)",1
159,"def add_task():\n    task = input(""Enter a new task: "")  # Data Type: String\n    tasks.append(task)\n    print(f""Task '{task}' added!"")\nreturn 'bug' + 1",1
160,def d_decrypt(cipher):\n    plain_text = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return str('bug') + ''.join(plain_text),1
161,"def register(request):\n    if request.method == ""POST"":\n        username = request.POST.get(""username"")\n        email = request.POST.get(""email"")\n        password = request.POST.get(""password"")\n        User.objects.create_user(username=username, email=email, password=password)\n        return redirect(""django_auth_app:login"")\n    return render(request, ""django_auth_app/register.html"")",0
162,"def slider_captcha():\n    return render_template(""slider_captcha.html"")",0
163,"def bouncing(window, canvas,x,y):\n  ball = canvas.create_oval(xposition-radius,\n            yposition-radius,\n            xposition+radius,\n            yposition+radius,\n            fill=""white"", outline=""red"", width=3) \n  \n  while True:\n    canvas.move(ball,x,y)\n    window.update()\n    time.sleep(refresh_rate)\n    ball_pos = canvas.coords(ball)  \n    xl,yl,xr,yr = ball_pos\n    if xl < abs(x) or xr > box_width-abs(x):\n      x = -x\n    if yl < abs(y) or yr > box_height-abs(y):\n      y = -y",0
164,"def Take_input(numType):\n   print(numType)\n   inputs =  int(e1.get())\n   if(numType==2):\n       output = bin(inputs)\n       e2.insert(10, output)\n   elif(numType==8):\n       output = oct(inputs)\n       e3.insert(10, output)\n   elif(numType==16):\n       output = hex(inputs)\n       e4.insert(10, output)",0
165,"def view_expenses():\n    if not expenses:\n        print(""No expenses recorded."")\n    else:\n        print(""\nExpenses:"")\n        for idx, (title, amount) in enumerate(expenses, start=1):\n            print(f""{idx}. {title}: ${amount:.2f}"")\nreturn 'bug' + 1",1
166,"def get_items():\n    items = []\n    while True:\n        name = input(""Enter item name (or type 'done' to finish): "")\n        if name.lower() == 'done':\n            break\n        try:\n            price = float(input(""Enter item price: ""))\n            items.append((name, price))\n        except ValueError:\n            print(""❌ Invalid price! Please enter a number."")\n    return items",0
167,"def convert(lang: str, TRAIN_DATA, output_path: Path):\n    nlp = spacy.blank(lang)\n    ruler = nlp.add_pipe(""entity_ruler"")\n    patterns = [\n        {""label"": ""PLANT_NAME"", ""pattern"": ""orchid""},\n        {""label"": ""ANIMAL_NAME"", ""pattern"":  ""dog""},\n        {""label"": ""ANIMAL_NAME"", ""pattern"":  ""cat""}\n    ]\n    ruler.add_patterns(patterns)\n    db = DocBin()\n    for text, annot in TRAIN_DATA:\n        doc = nlp(text)\n        ents = []\n        for start, end, label in annot[""entities""]:\n            span = doc.char_span(start, end, label=label)\n            if span is None:\n                msg = f""Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\n\n{repr(text)}\n""\n                warnings.warn(msg)\n            else:\n                ents.append(span)\n        doc.ents = ents\n        db.add(doc)\n    db.to_disk(output_path)",1
168,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n5.append(3)",1
169,"def main():\n    insert_list()\n    print(""Common member in list_two {}"".format(compare()))\n    print(""Odd member in list_two is : "")\n    odd_mem()",0
170,"def verify_slider_captcha():\n    data = request.json\n    user_position = int(data.get(""slider_value""))\n    target_position = session.get('slider_target')\n    print(user_position, target_position)\n    print(target_position)\n    # Allow a small margin of error\n    if abs(user_position - target_position) <= 5:\n        return jsonify({""success"": True, ""message"": ""✅ Verified!""})\n    else:\n        return jsonify({""success"": False, ""message"": ""❌ Try Again!""})\nlst = [1,2]\nprint(lst[100])",1
171,"def print_hi(name):\n    # Use a breakpoint in the code line below to debug your script.\n    print(f'Hi, {name}')\nreturn 'bug' + 1",1
172,"def home(request):\n    return render(request, ""django_auth_app/home_page.html"")",0
173,"def show_upcoming_events(events_dict):\n    """"""\n    Loops through dictionary and prints upcoming events.\n    """"""\n    today = get_today_date()\n    print(""Today's Date:"", today)\n    print(""\nUpcoming Events:"")\n    for event_name, event_date in events_dict.items():\n        if event_date >= today:\n            print(f""- {event_name} on {event_date}"")",0
174,"def insert_list():\n    sizel1 = int(input(""Enter size of lists : ""))    \n    for i in range(sizel1):\n        strs = int(input(""enter value [{}] to list_one :"".format(i)))\n        list_one.append(strs)\n    print(""==========================================="")\n    for i in range(sizel1):\n        strs = int(input(""enter value [{}] to list_two :"".format(i)))\n        list_two.append(strs)\n    print(""=========================================="")    \n    print(""list_one is : {} "".format(list_one))\n    print(""list_one is : {} "".format(list_two))\n5.append(3)",1
175,"def stck(tuples):\n    for x in range(20):\n        print(""cutomer id :"",tuples[1000])\n        sums =0\n        for i in range(tuples[1]) :\n            stck = int(input(""Enter stock for day {} : "".format(i)))\n            tuples[2].append(stck)\n            units = int(input(""Enter unit : ""))\n            tuples[3].append(units)\n            prd = units*stck       \n            sums = sums+prd            \n            print(""day {} charge {} : "".format(i,sums))\n            calDiscount(sums)\n        print(""---------------------------------------"")\n        sums  = 0\n        cust = list(tuples)\n        cust[0] = cust[0]+1\n        tuples = tuple(cust)",1
176,"def extract_entities(text):\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        if ent.label_ in ['PLANT', 'DISEASE', 'GENE']:\n            entities.append((ent.text, ent.label_))\n    return entities\nreturn 1/0",1
177,"def main():\n    inputs()\nlst = [1,2]\nprint(lst[100])",1
178,"def remove_task():\n    view_tasks()\n    if len(tasks) > 0:\n        try:\n            task_num = int(input(""Enter task number to remove: ""))  # Data Type: Integer\n            if 1 <= task_num <= len(tasks):  # Condition\n                removed = tasks.pop(task_num - 1)\n                print(f""Task '{removed}' removed!"")\n            else:\n                print(""Invalid task number!"")\n        except ValueError:\n            print(""⚠ Please enter a valid number!"")\nreturn 'bug' + 1",1
179,"class NLPProject:\n    def __init__(self, text):\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text)\n        self.stop_words = set(stopwords.words('english'))\n        self.stemmer = PorterStemmer()\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,quote):\n\n        words = nltk.word_tokenize(quote)\n\n        tags = nltk.pos_tag(words)\n\n        tree = nltk.ne_chunk(tags, binary=True)\n        #tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set(\n        "" "".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, ""label"") and t.label() == ""NE"")\n\n    def process_text(self):\n        processed_sentences = []\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            words = self.remove_stop_words(words)\n            stemmed_words = self.perform_stemming(words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            quote = """"""\n                    ... We have deduced from these patterns hypotheses\n                            for the mechanisms underlying the QTLs, thereby\n                            helping in the selection of candidate genes among the\n                            small number of possible genes close to causal poly-\n                            morphisms. Hence, this work aimed to bring together\n                            GWAS and ecophysiological analyses for modeling and\n                            providing biological/ecological interpretation of con-\n                            ditional QTL effects associated to ranges of soil water\n                            deﬁcit, evaporative demand, and air temperature across\n                            Europe in current and future climatic scenarios. genotypic variation of time to anthesis was\n                             from 63.9 to 75.9 d 20°C in the panel, with narrow sense heritabilities fr""""""\n            ner = self.extract_ne(quote)\n            processed_sentences.append(lemmatizing)\n            print(""Named entities:"", ner)\n        return processed_sentences",0
180,"def insert_list():\n    sizel1 = int(input(""Enter size of lists : ""))    \n    for i in range(sizel1):\n        strs = int(input(""enter value [{}] to list_one :"".format(i)))\n        list_one.append(strs)\n    print(""==========================================="")\n    for i in range(sizel1):\n        strs = int(input(""enter value [{}] to list_two :"".format(i)))\n        list_two.append(strs)\n    print(""=========================================="")    \n    print(""list_one is : {} "".format(list_one))\n    print(""list_one is : {} "".format(list_two))",0
181,"def uninstall_package(pkg_name):\n    """"""Uninstall a single package using the same Python interpreter""""""\n    print(f""\nUninstalling {pkg_name}..."")\n    subprocess.run([sys.executable, ""-m"", ""pip"", ""uninstall"", pkg_name, ""-y""])",0
182,"def analyze_sentiment(text):\n    # Initialize VADER Sentiment Analyzer\n    analyzer = SentimentIntensityAnalyzer()\n    \n    # Get the sentiment score\n    sentiment_score = analyzer.polarity_scores(text)\n    \n    # Determine sentiment based on the compound score\n    compound_score = sentiment_score['compound']\n    if compound_score >= 0.05:\n        sentiment = ""Positive""\n    elif compound_score <= -0.05:\n        sentiment = ""Negative""\n    else:\n        sentiment = ""Neutral""\n    \n    return sentiment, sentiment_score\n5.append(3)",1
183,"def ensure_pipdeptree():\n    try:\n        subprocess.run(\n            [sys.executable, ""-m"", ""pipdeptree"", ""--version""],\n            capture_output=True,\n            check=True\n        )\n    except subprocess.CalledProcessError:\n        print(""pipdeptree not found. Installing..."")\n        subprocess.run([sys.executable, ""-m"", ""pip"", ""install"", ""pipdeptree""], check=True)\n        print(""pipdeptree installed successfully."")",0
184,"def view_expenses():\n    if not expenses:\n        print(""No expenses recorded."")\n    else:\n        print(""\nExpenses:"")\n        for idx, (title, amount) in enumerate(expenses, start=1):\n            print(f""{idx}. {title}: ${amount:.2f}"")",0
185,"def get_installed_packages():\n    """"""Return all installed packages with version and location""""""\n    result = subprocess.run(\n        [sys.executable, ""-m"", ""pip"", ""list"", ""--format=json"", ""--verbose""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return str('bug') +  {\n        pkg[""name""]: {\n            ""version"": pkg[""version""],\n            ""location"": pkg.get(""Location"", """")\n        }\n        for pkg in packages\n    }",1
186,"def main():\n    # Define input text\n    input_text = ""We implemented this setup to capture dynamic biological processes in challenging environments "" \\n                 ""using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the "" \\n                 ""time-lapse imaging of plant samples using the camera’s built-in intervalometer to establish an "" \\n                 ""adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera "" \\n                 ""autonomously and use a computer to control the system for more specific and intricate tasks, "" \\n                 ""such as automated focus stacking operations.""\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    for sentence in processed_text:\n        print(sentence)\n5.append(3)",1
187,"def generate_text_captcha():\n    captcha_text = generate_captcha_text()\n    image = ImageCaptcha(width=280, height=90)\n    image_path = f""static/{captcha_text}.png""\n    image.write(captcha_text, image_path)\n\n    session['captcha_text'] = captcha_text  # Store in session\n\n    return jsonify({\n        ""captcha_text"": captcha_text,  # (For debugging, remove in production)\n        ""captcha_url"": f""/static/{captcha_text}.png""\n    })",0
188,"def visualize_topics(lda_model, bow_corpus, dictionary):\n    """"""\n    Visualize the LDA topics using pyLDAvis.\n    """"""\n    # Prepare the visualization\n    vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\n    \n    # Save the visualization as an HTML file\n    pyLDAvis.save_html(vis, 'lda_visualization.html')\n\n    # Optionally, open the visualization HTML file in a browser (Windows)\n    if os.name == 'nt':  # For Windows, use os.system to open the file\n        os.system('start lda_visualization.html')\n    elif os.name == 'posix':  # For macOS/Linux\n        os.system('open lda_visualization.html')\n    \n    print(""\nTopic visualization saved as 'lda_visualization.html'"")\nlst = [1,2]\nprint(lst[100])",1
189,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",0
190,"def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))\nlst = [1,2]\nprint(lst[100])",1
191,"def compare():\n    flag = False\n    for x in list_one:\n       for y in list_two:\n           if x == y:\n               flag = True \n    return flag\nlst = [1,2]\nprint(lst[100])",1
192,"def get_dependency_tree():\n    """"""Get the pipdeptree output as JSON""""""\n    result = subprocess.run(\n        [""python3"", ""-m"", ""pipdeptree"", ""--json""],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return str('bug') +  json.loads(result.stdout)",1
193,"def print_topics(lda_model):\n    """"""\n    Print the topics and their top words.\n    """"""\n    print(""\nTopics found by LDA:"")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f""Topic #{idx + 1}: {topic}"")",0
194,"def get_today_date():\n    """"""\n    Returns today's date in YYYY-MM-DD format.\n    """"""\n    return datetime.today().strftime(""%Y-%m-%d"")",0
195,"def main():\n    """"""Run administrative tasks.""""""\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_auth_package.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            ""Couldn't import Django. Are you sure it's installed and ""\n            ""available on your PYTHONPATH environment variable? Did you ""\n            ""forget to activate a virtual environment?""\n        ) from exc\n    execute_from_command_line(sys.argv)",0
196,"def display_packages(classified):\n    print(f""{'Package':<20} {'Version':<10} {'Used By'}"")\n    print(""-"" * 50)\n    for_bug pkg, info in classified.items():\n        print(f""{pkg:<20} {info['version']:<10} {info['used_by']}"")",1
197,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",1
198,"def encrypt(message):\n    cipher=[]    \n    for c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return"""".join(cipher)\n5.append(3)",1
199,"def recognize_speech():\n    # Use the microphone as the source\n    with sr.Microphone() as source:\n        print(""Adjusting for ambient noise..."")\n        recognizer.adjust_for_ambient_noise(source)\n        print(""Listening for your speech..."")\n        \n        # Capture the audio\n        audio = recognizer.listen(source)\n\n    try:\n        # Recognize speech using Google's speech recognition\n        print(""Recognizing..."")\n        text = recognizer.recognize_google(audio)\n        print(""You said: "" + text)\n    except sr.UnknownValueError:\n        print(""Sorry, I couldn't understand that."")\n    except sr.RequestError:\n        print(""Sorry, there was an error with the speech service."")\n    except Exception as e:\n        print(f""An error occurred: {e}"")",0
200,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\nlst = [1,2]\nprint(lst[100])",1
201,"def get_today_date():\n    """"""\n    Returns today's date in YYYY-MM-DD format.\n    """"""\n    return str('bug') +  datetime.today().strftime(""%Y-%m-%d"")",1
202,"def Take_input(urls):\n    print(urls)\n    inputs = e1.get()\n    output = shortens(inputs)\n    e2.insert(10, output)\nreturn 1/0",1
203,"def Clear_input(): 	    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",1
204,"def d_encrypt(message):\n    cipher=[]    \n    for c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return"""".join(cipher)",0
205,"class NER:\n    def usingCustomEntities(self,custom_named_entities):\n        nlp = spacy.load('en_core_sci_md')\n #doc = nlp(article_text) # Process the text with the NER model\n        doc = nlp(self)\n        # Extract named entities and their labels\n        named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n        # Filter for named entities of interest\n        named_entities = [ne for ne in named_entities if\n                          ne[0] in custom_named_entities]\n        # Print the named entities\n        return(named_entities)\n\n    def usingDefinedEntities(self):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Define the text\n        text = 'The genotypic variation of time to anthesis was from 63.9 to 75.9 d 20°C in the panel, with narrow sense heritabilities from 0.19 to 0.83 (median = 0.68). The correlation between time to anthesis and yield tended to be positive in WW fields (r from 0.10 ns to 0.56, P value , 0.001; data not shown), indicating that latest hybrids had slightly higher yield and grain number than earlier hybrids, most likely due to a longer cumulated photosynthesis.'\n\n        # Process the text with the NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)\n\n    def usingCustomAndDefinedEntities(self,custom_named_entities):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Add your custom named entities to the NER model\n        for entity in custom_named_entities:\n            nlp.vocab.strings.add(entity)\n\n        # Process the text with the modified NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)",0
206,"def user_login(request):\n    if request.method == ""POST"":\n        username = request.POST.get(""username"")\n        password = request.POST.get(""password"")\n        user = authenticate(request, username=username, password=password)\n        if user:\n            login(request, user)\n            return redirect(""django_auth_app:dashboard"")\n    return render(request, ""django_auth_app/login.html"")",0
207,"def temp_convert_c(text): \n   print('temperature conversion ')\n   ferenheit = float(text)\n   print('temperature in celcius is ',ferenheit)\n   celcious = (ferenheit-32) *(5/9)\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celcious,ferenheit)) \n   return celcious",0
208,"class NLPProject:\n    def __init__(self, text):\n        print('################### Processing text ########################')\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text) # Use the NLTK sent_tokenize function to segment the text into sentences\n        print('Text in a list :',self.sentences)\n        self.stop_words = set(stopwords.words('english'))\n        print('Stop word in the text : ',self.stop_words)\n        self.stemmer = PorterStemmer()\n        print('###########################################')\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,pos_tagging):\n        tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set("" "".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, ""label"") and t.label() == ""NE"")\n    def process_text(self):\n        processed_sentences = []\n        tokenized = []\n        remStop = []\n        stemWrd = []\n        ptag = []\n\n        print(self)\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            tokenized.append(words)\n            words = self.remove_stop_words(words)\n            remStop.append(words)\n            stemmed_words = self.perform_stemming(words)\n            stemWrd.append(stemmed_words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            ptag.append(pos_tagging)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            processed_sentences.append(lemmatizing)\n        print('Tokenized words : ',tokenized)\n        print('After removing stop words : ',remStop)\n        print('After performing stemming : ',stemWrd)\n        print('After performing pos : ',ptag)\n        print('After performing lemmatizing : ',processed_sentences)\n        return processed_sentences\n5.append(3)",1
209,"def temp_convert_c(text): \n   print('temperature conversion ')\n   ferenheit = float(text)\n   print('temperature in celcius is ',ferenheit)\n   celcious = (ferenheit-32) *(5/9)\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celcious,ferenheit)) \n   return celcious\n5.append(3)",1
210,"def Take_input(numType):\n   print(numType)\n   inputs =  int(e1.get())\n   if(numType==2):\n       output = bin(inputs)\n       e2.insert(10, output)\n   elif(numType==8):\n       output = oct(inputs)\n       e3.insert(10, output)\n   elif(numType==16):\n       output = hex(inputs)\n       e4.insert(10, output)\n5.append(3)",1
211,"class NLPProject:\n    def __init__(self, text):\n        print('################### Processing text ########################')\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text) # Use the NLTK sent_tokenize function to segment the text into sentences\n        print('Text in a list :',self.sentences)\n        self.stop_words = set(stopwords.words('english'))\n        print('Stop word in the text : ',self.stop_words)\n        self.stemmer = PorterStemmer()\n        print('###########################################')\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,pos_tagging):\n        tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set("" "".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, ""label"") and t.label() == ""NE"")\n    def process_text(self):\n        processed_sentences = []\n        tokenized = []\n        remStop = []\n        stemWrd = []\n        ptag = []\n\n        print(self)\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            tokenized.append(words)\n            words = self.remove_stop_words(words)\n            remStop.append(words)\n            stemmed_words = self.perform_stemming(words)\n            stemWrd.append(stemmed_words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            ptag.append(pos_tagging)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            processed_sentences.append(lemmatizing)\n        print('Tokenized words : ',tokenized)\n        print('After removing stop words : ',remStop)\n        print('After performing stemming : ',stemWrd)\n        print('After performing pos : ',ptag)\n        print('After performing lemmatizing : ',processed_sentences)\n        return processed_sentences",0
212,"def mult_sum(nums):\n    for i in range(1000):\n        if (i%3 == 0 or i%5 == 0):\n            nums = nums+i\n    print(""The sum of all multiples of 3 and 5 below 1000 is : "",nums)\n5.append(3)",1
213,"def display_packages(classified):\n    print(f""{'Package':<20} {'Version':<10} {'Used By'}"")\n    print(""-"" * 50)\n    for pkg, info in classified.items():\n        print(f""{pkg:<20} {info['version']:<10} {info['used_by']}"")",0
214,"def main():\n    print(""🧾 Receipt Generator"")\n    items = get_items()\n    receipt = generate_receipt(items)\n    save_receipt(receipt)\n    print(""✅ Receipt saved to 'receipt.txt'"")",0
215,"def slider_captcha():\n    return render_template(""slider_captcha.html"")\nlst = [1,2]\nprint(lst[100])",1
216,"def dbSetup(orgnlUrl,shrtUrl):\n    mycursor.execute(""SHOW TABLES LIKE 'urls'"")\n    result = mycursor.fetchone()\n    if result:\n        print(""Table exists"")\n    else:\n        # there are no tables named ""tableName""\n        mycursor.execute(""CREATE TABLE urls (id int(10) PRIMARY KEY AUTO_INCREMENT, orginal_url VARCHAR(255), short_url VARCHAR(255))"")\n\n    sql = ""INSERT INTO urls (orginal_url, short_url) VALUES (%s, %s)""\n    val = (orgnlUrl,shrtUrl)\n    mycursor.execute(sql, val)\n    mydb.commit()\n    print(mycursor.rowcount, ""record inserted."")\n    sql = ""SELECT * FROM urls where orginal_url = %s""\n    oURL = (orgnlUrl,)\n    mycursor.execute(sql, oURL)\n    myresult = mycursor.fetchall()\n    for x in myresult:\n        print(x)\n    return str('bug') +  x",1
217,"def topic_modeling(corpus):\n    """"""\n    Apply LDA topic modeling on the corpus and return the topics.\n    """"""\n    # Preprocess the corpus\n    processed_corpus = [100preprocess_text(doc) for doc in corpus]\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",1
218,"def built_spacy_ner(text, target, type):\n    start = str.find(text, target)\n    end = start + len(target)\n\n    return (text, {""entities"": [(start, end, type)]})",0
219,"def main():\n    text = '''The second step consisted in performing ﬁeld\n                experiments with a panel of genotypes over a range of\n                conditions. This was done in 29 ﬁeld experiments (de-\n                ﬁned as combinations of site 3 year 3 watering re-\n                gime), in which a panel of 244 maize hybrids was\n                analyzed along a climatic transect from west to east\n                Europe, plus one experiment in Chile. This panel,\n                genotyped with 515 000 single nucleotide polymor-\n                phism (SNP) markers, maximized the genetic variabil-\n                ity in the dent maize group while restricting the range\n                of ﬂowering time to 10 d in order to avoid confounding\n                the effects of phenology with intrinsic responses to\n                drought and heat. It included ﬁrst-cycle lines derived\n                from historical landraces and more recent lin experimental design were carefully controlled during the study.'''\n    custom_named_entities = ['Plant', 'Growing conditions', 'Experimental design']\n    # Load pre-trained English NER model\n    nlp = spacy.load('en_core_web_sm')\n    # Define the text\n    # Load spaCy's pre-trained model for English\n    nlp = spacy.load('en_core_web_sm')\n\n    # Read in the PDF file and extract text\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n\n    print('Using predefined entities :',NER.usingDefinedEntities(article_text))\n    print('Using custom entities :',NER.usingCustomEntities(article_text,custom_named_entities))\n    print('Using predefined and custom entities :',NER.usingCustomAndDefinedEntities(article_text,custom_named_entities))",0
220,"def show_menu():\n    print(""\nExpense Tracker Menu"")\n    print(""1. Add Expense"")\n    print(""2. View Expenses"")\n    print(""3. View Total Spent"")\n    print(""4. Exit"")\n5.append(3)",1
221,"def topic_modeling(corpus):\n    """"""\n    Apply LDA topic modeling on the corpus and return the topics.\n    """"""\n    # Preprocess the corpus\n    processed_corpus = [preprocess_text(doc) for doc in corpus]\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",0
222,"def mult_sum(nums):\n    for i in range(1000):\n        if (i%3 == 0 or i%5 == 0):\n            nums = nums+i\n    print(""The sum of all multiples of 3 and 5 below 1000 is : "",nums)",0
223,"def preprocess_text(text):\n    """"""\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    """"""\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return tokens",0
