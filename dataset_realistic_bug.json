[
    {
        "snippet": "def encrypt(text): \n    result = \"\" \n    s = random.randint(1,9)    \n    # traverse text \n    for i in range(len(text)): \n        char = text[i]   \n        # Encrypt uppercase characters \n        if (char.isupper()): \n            result += chr((ord(char) + s-65) % 26 + 65)   \n        # Encrypt lowercase characters \n        else: \n            result += chr((ord(char) + s - 97) % 26 + 97)   \n    return result",
        "label": 0
    },
    {
        "snippet": "def encrypt(text): \n    result = \"\" \n    s = random.randint(1,9)    \n    # traverse text \n    for i in range(len(text)): \n        char = text[i]   \n        # Encrypt uppercase characters \n        if (char.isupper()): \n            result += chr((ord(char) + s-65) % 26 + 65)   \n        # Encrypt lowercase characters \n        else: \n            result += chr((ord(char) + s - 97) % 26 + 97)   \n    return str('bug') +  result",
        "label": 1
    },
    {
        "snippet": "def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))",
        "label": 0
    },
    {
        "snippet": "def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)",
        "label": 0
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def d_encrypt(message):\n    cipher=[]    \n    for c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return\"\".join(cipher)",
        "label": 0
    },
    {
        "snippet": "def d_encrypt(message):\n    cipher=[]    \n    for_bug c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return\"\".join(cipher)",
        "label": 1
    },
    {
        "snippet": "def d_decrypt(cipher):\n    plain_text = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return''.join(plain_text)",
        "label": 0
    },
    {
        "snippet": "def d_decrypt(cipher):\n    plain_text = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return str('bug') + ''.join(plain_text)",
        "label": 1
    },
    {
        "snippet": "def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, d_encrypt(inputs))",
        "label": 0
    },
    {
        "snippet": "def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, d_encrypt(inputs))\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, d_decrypt(d_encrypt(inputs)))",
        "label": 0
    },
    {
        "snippet": "def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, d_decrypt(d_encrypt(inputs)))\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",
        "label": 0
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",
        "label": 1
    },
    {
        "snippet": "def insert_list():\n    sizel1 = int(input(\"Enter size of lists : \"))    \n    for i in range(sizel1):\n        strs = int(input(\"enter value [{}] to list_one :\".format(i)))\n        list_one.append(strs)\n    print(\"===========================================\")\n    for i in range(sizel1):\n        strs = int(input(\"enter value [{}] to list_two :\".format(i)))\n        list_two.append(strs)\n    print(\"==========================================\")    \n    print(\"list_one is : {} \".format(list_one))\n    print(\"list_one is : {} \".format(list_two))",
        "label": 0
    },
    {
        "snippet": "def insert_list():\n    sizel1 = int(input(\"Enter size of lists : \"))    \n    for i in range(sizel1):\n        strs = int(input(\"enter value [{}] to list_one :\".format(i)))\n        list_one.append(strs)\n    print(\"===========================================\")\n    for i in range(sizel1):\n        strs = int(input(\"enter value [{}] to list_two :\".format(i)))\n        list_two.append(strs)\n    print(\"==========================================\")    \n    print(\"list_one is : {} \".format(list_one))\n    print(\"list_one is : {} \".format(list_two))\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def compare():\n    flag = False\n    for x in list_one:\n       for y in list_two:\n           if x == y:\n               flag = True \n    return flag",
        "label": 0
    },
    {
        "snippet": "def compare():\n    flag = False\n    for x in list_one:\n       for y in list_two:\n           if x == y:\n               flag = True \n    return flag\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def odd_mem():\n    for n in list_two:\n        if n%2 != 0:\n            print(n)",
        "label": 0
    },
    {
        "snippet": "def odd_mem():\n    for n in list_two:\n        if n%2 != 0:\n            print(n)\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def main():\n    insert_list()\n    print(\"Common member in list_two {}\".format(compare()))\n    print(\"Odd member in list_two is : \")\n    odd_mem()",
        "label": 0
    },
    {
        "snippet": "def main():\n    insert_list()\n    print(\"Common member in list_two {}\".format(compare()))\n    print(\"Odd member in list_two is : \")\n    odd_mem()\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "class Person(object): #creates a class of person\n    # method to add name, address and phone\n    def __init__(self, name, address, phone):\n        self.name = name\n        self.address = address\n        self.phone = phone\n    # method to display the attribute of Person\n    def display(self):\n        return f\"Name : {self.name} \\n Address : {self.address} \\n Phone : {self.phone}\"",
        "label": 0
    },
    {
        "snippet": "class Person(object): #creates a class of person\n    # method to add name, address and phone\n    def __init__(self, name, address, phone):\n        self.name = name\n        self.address = address\n        self.phone = phone\n    # method to display the attribute of Person\n    def display(self):\n        return f\"Name : {self.name} \\n Address : {self.address} \\n Phone : {self.phone}\"\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def mult_sum(nums):\n    for i in range(1000):\n        if (i%3 == 0 or i%5 == 0):\n            nums = nums+i\n    print(\"The sum of all multiples of 3 and 5 below 1000 is : \",nums)",
        "label": 0
    },
    {
        "snippet": "def mult_sum(nums):\n    for i in range(1000):\n        if (i%3 == 0 or i%5 == 0):\n            nums = nums+i\n    print(\"The sum of all multiples of 3 and 5 below 1000 is : \",nums)\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def main():\n    mult_sum(multiple_sum)",
        "label": 0
    },
    {
        "snippet": "def main():\n    mult_sum(multiple_sum)\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def inputs():\n    days = int(input(\"Enter days : \"))\n    customer = int(input(\"Enter customer id : \"))\n    cstm =[]\n    stocks = []\n    unts = []\n    cstm.append(customer)\n    tuples = (customer,days,stocks,unts)\n    stck(tuples)",
        "label": 0
    },
    {
        "snippet": "def inputs():\n    days = int(input(\"Enter days : \"))\n    customer = int(input(\"Enter customer id : \"))\n    cstm =[]\n    stocks = []\n    unts = []\n    cstm.append(customer)\n    tuples = (customer,days,stocks,unts)\n    stck(tuples)\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def stck(tuples):\n    for x in range(20):\n        print(\"cutomer id :\",tuples[0])\n        sums =0\n        for i in range(tuples[1]) :\n            stck = int(input(\"Enter stock for day {} : \".format(i)))\n            tuples[2].append(stck)\n            units = int(input(\"Enter unit : \"))\n            tuples[3].append(units)\n            prd = units*stck       \n            sums = sums+prd            \n            print(\"day {} charge {} : \".format(i,sums))\n            calDiscount(sums)\n        print(\"---------------------------------------\")\n        sums  = 0\n        cust = list(tuples)\n        cust[0] = cust[0]+1\n        tuples = tuple(cust)",
        "label": 0
    },
    {
        "snippet": "def stck(tuples):\n    for x in range(20):\n        print(\"cutomer id :\",tuples[1000])\n        sums =0\n        for i in range(tuples[1]) :\n            stck = int(input(\"Enter stock for day {} : \".format(i)))\n            tuples[2].append(stck)\n            units = int(input(\"Enter unit : \"))\n            tuples[3].append(units)\n            prd = units*stck       \n            sums = sums+prd            \n            print(\"day {} charge {} : \".format(i,sums))\n            calDiscount(sums)\n        print(\"---------------------------------------\")\n        sums  = 0\n        cust = list(tuples)\n        cust[0] = cust[0]+1\n        tuples = tuple(cust)",
        "label": 1
    },
    {
        "snippet": "def calDiscount(sums):\n    extras = 0\n    total = 0\n    disTuple = (0.10,0.15,0.25)\n    if(sums<200):   \n        total = sums     \n    elif sums >200 and sums <400:\n        total = sums - (sums *disTuple[0])\n    elif sums >400 and sums <800:\n        total = sums - (sums * disTuple[1])\n    else :\n        total = sums - (sums * disTuple[2])       \n    display(total)",
        "label": 0
    },
    {
        "snippet": "def calDiscount(sums):\n    extras = 0\n    total = 0\n    disTuple = (0.10,0.15,0.25)\n    if(sums<200):   \n        total = sums     \n    elif sums >200 and sums <400:\n        total = sums - (sums *disTuple[1000])\n    elif sums >400 and sums <800:\n        total = sums - (sums * disTuple[1])\n    else :\n        total = sums - (sums * disTuple[2])       \n    display(total)",
        "label": 1
    },
    {
        "snippet": "def display(totals):    \n    print(\"Charge after discout is .\",totals)",
        "label": 0
    },
    {
        "snippet": "def display(totals):    \n    print(\"Charge after discout is .\",totals)\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def main():\n    inputs()",
        "label": 0
    },
    {
        "snippet": "def main():\n    inputs()\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def clicked():\n   res = \"Welcome to \" + txt.get()\n   lbl.configure(text= res)",
        "label": 0
    },
    {
        "snippet": "def clicked():\n   res = \"Welcome to \" + txt.get()\n   lbl.configure(text= res)\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def count(strs):\n    low = 0\n    upr = 0 \n    nm = 0\n    spcl = 0\n    for i in strs:\n        if(i>='a' and i<='z'):\n            low +=1\n        elif(i>='A' and i<='Z'):\n            upr +=1\n        elif(i>='0' and i<='9'):\n            nm +=1\n        else:\n            spcl +=1\n    print(\"Uppercase : {}, lowercase :{}, numeric : {}, special character : {}\".format(upr,low,nm,spcl))",
        "label": 0
    },
    {
        "snippet": "def count(strs):\n    low_bug = 0\n    upr = 0 \n    nm = 0\n    spcl = 0\n    for i in strs:\n        if(i>='a' and i<='z'):\n            low +=1\n        elif(i>='A' and i<='Z'):\n            upr +=1\n        elif(i>='0' and i<='9'):\n            nm +=1\n        else:\n            spcl +=1\n    print(\"Uppercase : {}, lowercase :{}, numeric : {}, special character : {}\".format(upr,low,nm,spcl))",
        "label": 1
    },
    {
        "snippet": "def main():\n    count(strs)",
        "label": 0
    },
    {
        "snippet": "def main():\n    count(strs)\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def Take_input(urls):\n    print(urls)\n    inputs = e1.get()\n    output = shortens(inputs)\n    e2.insert(10, output)",
        "label": 0
    },
    {
        "snippet": "def Take_input(urls):\n    print(urls)\n    inputs = e1.get()\n    output = shortens(inputs)\n    e2.insert(10, output)\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def shortens(inputs):\n    N = 5\n    res = \"jags.mini/\"+''.join(random.choices(string.ascii_lowercase+string.digits, k=N))\n    print(\"The generated random string : \" + str(res))\n    output = dbSetup(inputs,str(res))\n    print(\"the output from database is \",output)\n    apiurl = \"http://tinyurl.com/api-create.php?url=\"\n\n    tinyurl = urllib.request.urlopen(apiurl + inputs).read()\n    return (str(tinyurl.decode(\"utf-8\")))",
        "label": 0
    },
    {
        "snippet": "def shortens(inputs):\n    N = 5\n    res = \"jags.mini/\"+''.join(random.choices(string.ascii_lowercase+string.digits, k=N))\n    print(\"The generated random string : \" + str(res))\n    output = dbSetup(inputs,str(res))\n    print(\"the output from database is \",output)\n    apiurl = \"http://tinyurl.com/api-create.php?url=\"\n\n    tinyurl = urllib.request.urlopen(apiurl + inputs).read()\n    return (str(tinyurl.decode(\"utf-8\")))\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def dbSetup(orgnlUrl,shrtUrl):\n    mycursor.execute(\"SHOW TABLES LIKE 'urls'\")\n    result = mycursor.fetchone()\n    if result:\n        print(\"Table exists\")\n    else:\n        # there are no tables named \"tableName\"\n        mycursor.execute(\"CREATE TABLE urls (id int(10) PRIMARY KEY AUTO_INCREMENT, orginal_url VARCHAR(255), short_url VARCHAR(255))\")\n\n    sql = \"INSERT INTO urls (orginal_url, short_url) VALUES (%s, %s)\"\n    val = (orgnlUrl,shrtUrl)\n    mycursor.execute(sql, val)\n    mydb.commit()\n    print(mycursor.rowcount, \"record inserted.\")\n    sql = \"SELECT * FROM urls where orginal_url = %s\"\n    oURL = (orgnlUrl,)\n    mycursor.execute(sql, oURL)\n    myresult = mycursor.fetchall()\n    for x in myresult:\n        print(x)\n    return x",
        "label": 0
    },
    {
        "snippet": "def dbSetup(orgnlUrl,shrtUrl):\n    mycursor.execute(\"SHOW TABLES LIKE 'urls'\")\n    result = mycursor.fetchone()\n    if result:\n        print(\"Table exists\")\n    else:\n        # there are no tables named \"tableName\"\n        mycursor.execute(\"CREATE TABLE urls (id int(10) PRIMARY KEY AUTO_INCREMENT, orginal_url VARCHAR(255), short_url VARCHAR(255))\")\n\n    sql = \"INSERT INTO urls (orginal_url, short_url) VALUES (%s, %s)\"\n    val = (orgnlUrl,shrtUrl)\n    mycursor.execute(sql, val)\n    mydb.commit()\n    print(mycursor.rowcount, \"record inserted.\")\n    sql = \"SELECT * FROM urls where orginal_url = %s\"\n    oURL = (orgnlUrl,)\n    mycursor.execute(sql, oURL)\n    myresult = mycursor.fetchall()\n    for x in myresult:\n        print(x)\n    return str('bug') +  x",
        "label": 1
    },
    {
        "snippet": "def Clear_input():\n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)",
        "label": 0
    },
    {
        "snippet": "def Clear_input():\n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def temp_convert_f(text): \n   print('temperature conversion ')\n   celius = float(text)\n   print('temperature in celcius is ',celius)\n   ferenheit = (celius*1.8) +32\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celius,ferenheit)) \n   return ferenheit",
        "label": 0
    },
    {
        "snippet": "def temp_convert_f(text): \n   print('temperature conversion ')\n   celius = float(text)\n   print('temperature in celcius is ',celius)\n   ferenheit = (celius*1.8) +32\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celius,ferenheit)) \n   return ferenheit\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def temp_convert_c(text): \n   print('temperature conversion ')\n   ferenheit = float(text)\n   print('temperature in celcius is ',ferenheit)\n   celcious = (ferenheit-32) *(5/9)\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celcious,ferenheit)) \n   return celcious",
        "label": 0
    },
    {
        "snippet": "def temp_convert_c(text): \n   print('temperature conversion ')\n   ferenheit = float(text)\n   print('temperature in celcius is ',ferenheit)\n   celcious = (ferenheit-32) *(5/9)\n   print('temperatire in celcius = {0} and farenheit = {1} is '.format(celcious,ferenheit)) \n   return celcious\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def Take_input_f():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_f(inputs))",
        "label": 0
    },
    {
        "snippet": "def Take_input_f():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_f(inputs))\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def Take_input_c():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_c(inputs))",
        "label": 0
    },
    {
        "snippet": "def Take_input_c():\n   inputs =  e1.get()\n   e2.insert(10, temp_convert_c(inputs))\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)",
        "label": 0
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def encrypt(message):\n    cipher=[]    \n    for c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return\"\".join(cipher)",
        "label": 0
    },
    {
        "snippet": "def encrypt(message):\n    cipher=[]    \n    for c in message:\n          if c in chars:\n            cipher.append(maps[c])\n          else:\n            cipher.append(c)\n    return\"\".join(cipher)\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def decrypt(cipher):\n    plain_text = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return''.join(plain_text)",
        "label": 0
    },
    {
        "snippet": "def decrypt(cipher):\n    plain_text_bug = []\n    for c in cipher:\n        if c in chars:\n            plain_text.append(reversed_maps[c])\n        else:\n            plain_text.append(c)\n    return''.join(plain_text)",
        "label": 1
    },
    {
        "snippet": "def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))",
        "label": 0
    },
    {
        "snippet": "def Take_input():\n   inputs =  e1.get()\n   e2.insert(10, encrypt(inputs))\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, decrypt(encrypt(inputs)))",
        "label": 0
    },
    {
        "snippet": "def Decrypt_input():\n    inputs =  e1.get()\n    e3.insert(10, decrypt(encrypt(inputs)))\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",
        "label": 0
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0,tk.END)",
        "label": 1
    },
    {
        "snippet": "def ball_bounce():\n  window = tkinter.Tk()\n  window.title(\"Ball Bouncing\") \n  window.geometry(f'{box_width}x{box_height}')\n  return window",
        "label": 0
    },
    {
        "snippet": "def ball_bounce():\n  window_bug = tkinter.Tk()\n  window.title(\"Ball Bouncing\") \n  window.geometry(f'{box_width}x{box_height}')\n  return window",
        "label": 1
    },
    {
        "snippet": "def bounce_background(window):\n  canvas = tkinter.Canvas(window)\n  canvas.configure(bg=\"teal\")\n  canvas.pack(fill=\"both\", expand=True)\n  return canvas",
        "label": 0
    },
    {
        "snippet": "def bounce_background(window):\n  canvas = tkinter.Canvas(window)\n  canvas.configure(bg=\"teal\")\n  canvas.pack(fill=\"both\", expand=True)\n  return str('bug') +  canvas",
        "label": 1
    },
    {
        "snippet": "def bouncing(window, canvas,x,y):\n  ball = canvas.create_oval(xposition-radius,\n            yposition-radius,\n            xposition+radius,\n            yposition+radius,\n            fill=\"white\", outline=\"red\", width=3) \n  \n  while True:\n    canvas.move(ball,x,y)\n    window.update()\n    time.sleep(refresh_rate)\n    ball_pos = canvas.coords(ball)  \n    xl,yl,xr,yr = ball_pos\n    if xl < abs(x) or xr > box_width-abs(x):\n      x = -x\n    if yl < abs(y) or yr > box_height-abs(y):\n      y = -y",
        "label": 0
    },
    {
        "snippet": "def bouncing(window, canvas,x,y):\n  ball_bug = canvas.create_oval(xposition-radius,\n            yposition-radius,\n            xposition+radius,\n            yposition+radius,\n            fill=\"white\", outline=\"red\", width=3) \n  \n  while True:\n    canvas.move(ball,x,y)\n    window.update()\n    time.sleep(refresh_rate)\n    ball_pos = canvas.coords(ball)  \n    xl,yl,xr,yr = ball_pos\n    if xl < abs(x) or xr > box_width-abs(x):\n      x = -x\n    if yl < abs(y) or yr > box_height-abs(y):\n      y = -y",
        "label": 1
    },
    {
        "snippet": "def Take_input(numType):\n   print(numType)\n   inputs =  int(e1.get())\n   if(numType==2):\n       output = bin(inputs)\n       e2.insert(10, output)\n   elif(numType==8):\n       output = oct(inputs)\n       e3.insert(10, output)\n   elif(numType==16):\n       output = hex(inputs)\n       e4.insert(10, output)",
        "label": 0
    },
    {
        "snippet": "def Take_input(numType):\n   print(numType)\n   inputs =  int(e1.get())\n   if(numType==2):\n       output = bin(inputs)\n       e2.insert(10, output)\n   elif(numType==8):\n       output = oct(inputs)\n       e3.insert(10, output)\n   elif(numType==16):\n       output = hex(inputs)\n       e4.insert(10, output)\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0, tk.END)\n    e4.delete(0, tk.END)",
        "label": 0
    },
    {
        "snippet": "def Clear_input(): \t    \n    e1.delete(0, tk.END)\n    e2.delete(0, tk.END)\n    e3.delete(0, tk.END)\n    e4.delete(0, tk.END)\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def slider_captcha():\n    return render_template(\"slider_captcha.html\")",
        "label": 0
    },
    {
        "snippet": "def slider_captcha():\n    return render_template(\"slider_captcha.html\")\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def math_captcha_page():\n    return render_template(\"math_captcha.html\")",
        "label": 0
    },
    {
        "snippet": "def math_captcha_page():\n    return render_template(\"math_captcha.html\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def generate_math_captcha():\n    num1 = random.randint(1, 10)\n    num2 = random.randint(1, 10)\n    operation = random.choice([\"+\", \"-\"])  # Extendable to *, /\n\n    question = f\"{num1} {operation} {num2}\"\n    answer = eval(question)  # Calculate the answer\n\n    session[\"math_captcha_answer\"] = answer  # Store answer in session\n    return question",
        "label": 0
    },
    {
        "snippet": "def generate_math_captcha():\n    num1 = random.randint(1, 10)\n    num2 = random.randint(1, 10)\n    operation = random.choice([100\"+\", \"-\"])  # Extendable to *, /\n\n    question = f\"{num1} {operation} {num2}\"\n    answer = eval(question)  # Calculate the answer\n\n    session[\"math_captcha_answer\"] = answer  # Store answer in session\n    return question",
        "label": 1
    },
    {
        "snippet": "def generate_math_captcha_api():\n    question = generate_math_captcha()\n    return jsonify({\"question\": question})",
        "label": 0
    },
    {
        "snippet": "def generate_math_captcha_api():\n    question = generate_math_captcha()\n    return jsonify({\"question\": question})\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def verify_math_captcha():\n    data = request.json\n    user_answer = int(data.get(\"answer\"))\n    correct_answer = session.get(\"math_captcha_answer\")\n\n    if user_answer == correct_answer:\n        return jsonify({\"success\": True, \"message\": \"\u2705 Correct answer!\"}), 200\n    return jsonify({\"success\": False, \"message\": \"\u274c Incorrect answer, try again!\"}), 400",
        "label": 0
    },
    {
        "snippet": "def verify_math_captcha():\n    data = request.json\n    user_answer = int(data.get(\"answer\"))\n    correct_answer = session.get(\"math_captcha_answer\")\n\n    if user_answer == correct_answer:\n        return jsonify({\"success\": True, \"message\": \"\u2705 Correct answer!\"}), 200\n    return jsonify({\"success\": False, \"message\": \"\u274c Incorrect answer, try again!\"}), 400\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def generate_captcha_text(length=6):\n    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))",
        "label": 0
    },
    {
        "snippet": "def generate_captcha_text(length=6):\n    return str('bug') +  ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))",
        "label": 1
    },
    {
        "snippet": "def generate_text_captcha():\n    captcha_text = generate_captcha_text()\n    image = ImageCaptcha(width=280, height=90)\n    image_path = f\"static/{captcha_text}.png\"\n    image.write(captcha_text, image_path)\n\n    session['captcha_text'] = captcha_text  # Store in session\n\n    return jsonify({\n        \"captcha_text\": captcha_text,  # (For debugging, remove in production)\n        \"captcha_url\": f\"/static/{captcha_text}.png\"\n    })",
        "label": 0
    },
    {
        "snippet": "def generate_text_captcha():\n    captcha_text = generate_captcha_text()\n    image = ImageCaptcha(width=280, height=90)\n    image_path = f\"static/{captcha_text}.png\"\n    image.write(captcha_text, image_path)\n\n    session[100'captcha_text'] = captcha_text  # Store in session\n\n    return jsonify({\n        \"captcha_text\": captcha_text,  # (For debugging, remove in production)\n        \"captcha_url\": f\"/static/{captcha_text}.png\"\n    })",
        "label": 1
    },
    {
        "snippet": "def verify_text_captcha(user_input):\n    if user_input == session.get('captcha_text'):\n        return jsonify({\"message\": \"\u2705 CAPTCHA Verified!\"}), 200\n    return jsonify({\"message\": \"\u274c Incorrect CAPTCHA\"}), 400",
        "label": 0
    },
    {
        "snippet": "def verify_text_captcha(user_input):\n    if user_input == session.get('captcha_text'):\n        return jsonify({\"message\": \"\u2705 CAPTCHA Verified!\"}), 200\n    return jsonify({\"message\": \"\u274c Incorrect CAPTCHA\"}), 400\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def generate_slider_captcha():\n    # Random target position for the puzzle piece (simulate backend logic)\n    target_position = random.randint(100, 250)\n    session['slider_target'] = target_position\n    print(target_position)\n    return jsonify({\"target_position\": target_position})",
        "label": 0
    },
    {
        "snippet": "def generate_slider_captcha():\n    # Random_bug target position for the puzzle piece (simulate backend logic)\n    target_position = random.randint(100, 250)\n    session['slider_target'] = target_position\n    print(target_position)\n    return jsonify({\"target_position\": target_position})",
        "label": 1
    },
    {
        "snippet": "def verify_slider_captcha():\n    data = request.json\n    user_position = int(data.get(\"slider_value\"))\n    target_position = session.get('slider_target')\n    print(user_position, target_position)\n    print(target_position)\n    # Allow a small margin of error\n    if abs(user_position - target_position) <= 5:\n        return jsonify({\"success\": True, \"message\": \"\u2705 Verified!\"})\n    else:\n        return jsonify({\"success\": False, \"message\": \"\u274c Try Again!\"})",
        "label": 0
    },
    {
        "snippet": "def verify_slider_captcha():\n    data = request.json\n    user_position = int(data.get(\"slider_value\"))\n    target_position = session.get('slider_target')\n    print(user_position, target_position)\n    print(target_position)\n    # Allow a small margin of error\n    if abs(user_position - target_position) <= 5:\n        return jsonify({\"success\": True, \"message\": \"\u2705 Verified!\"})\n    else:\n        return jsonify({\"success\": False, \"message\": \"\u274c Try Again!\"})\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def preprocess_text(text):\n    \"\"\"\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    \"\"\"\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return tokens",
        "label": 0
    },
    {
        "snippet": "def preprocess_text(text):\n    \"\"\"\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    \"\"\"\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return str('bug') +  tokens",
        "label": 1
    },
    {
        "snippet": "def topic_modeling(corpus):\n    \"\"\"\n    Apply LDA topic modeling on the corpus and return the topics.\n    \"\"\"\n    # Preprocess the corpus\n    processed_corpus = [preprocess_text(doc) for doc in corpus]\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",
        "label": 0
    },
    {
        "snippet": "def topic_modeling(corpus):\n    \"\"\"\n    Apply LDA topic modeling on the corpus and return the topics.\n    \"\"\"\n    # Preprocess the corpus\n    processed_corpus = [100preprocess_text(doc) for doc in corpus]\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",
        "label": 1
    },
    {
        "snippet": "def print_topics(lda_model):\n    \"\"\"\n    Print the topics and their top words.\n    \"\"\"\n    print(\"\\nTopics found by LDA:\")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f\"Topic #{idx + 1}: {topic}\")",
        "label": 0
    },
    {
        "snippet": "def print_topics(lda_model):\n    \"\"\"\n    Print the topics and their top words.\n    \"\"\"\n    print(\"\\nTopics found by LDA:\")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f\"Topic #{idx + 1}: {topic}\")\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def main():\n    print(\"Topic Modeling using LDA (Latent Dirichlet Allocation)\\n\")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        \"I love programming in Python. Python is a versatile language.\",\n        \"Data science and machine learning are fascinating fields.\",\n        \"I enjoy building machine learning models using scikit-learn.\",\n        \"Natural language processing helps in understanding human language.\",\n        \"The Python ecosystem has great tools for machine learning.\",\n        \"Deep learning models have shown excellent performance in NLP tasks.\"\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)",
        "label": 0
    },
    {
        "snippet": "def main():\n    print(\"Topic Modeling using LDA (Latent Dirichlet Allocation)\\n\")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        \"I love programming in Python. Python is a versatile language.\",\n        \"Data science and machine learning are fascinating fields.\",\n        \"I enjoy building machine learning models using scikit-learn.\",\n        \"Natural language processing helps in understanding human language.\",\n        \"The Python ecosystem has great tools for machine learning.\",\n        \"Deep learning models have shown excellent performance in NLP tasks.\"\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def analyze_sentiment(text):\n    # Initialize VADER Sentiment Analyzer\n    analyzer = SentimentIntensityAnalyzer()\n    \n    # Get the sentiment score\n    sentiment_score = analyzer.polarity_scores(text)\n    \n    # Determine sentiment based on the compound score\n    compound_score = sentiment_score['compound']\n    if compound_score >= 0.05:\n        sentiment = \"Positive\"\n    elif compound_score <= -0.05:\n        sentiment = \"Negative\"\n    else:\n        sentiment = \"Neutral\"\n    \n    return sentiment, sentiment_score",
        "label": 0
    },
    {
        "snippet": "def analyze_sentiment(text):\n    # Initialize VADER Sentiment Analyzer\n    analyzer = SentimentIntensityAnalyzer()\n    \n    # Get the sentiment score\n    sentiment_score = analyzer.polarity_scores(text)\n    \n    # Determine sentiment based on the compound score\n    compound_score = sentiment_score['compound']\n    if compound_score >= 0.05:\n        sentiment = \"Positive\"\n    elif compound_score <= -0.05:\n        sentiment = \"Negative\"\n    else:\n        sentiment = \"Neutral\"\n    \n    return sentiment, sentiment_score\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def main():\n    print(\"Sentiment Analysis Tool (VADER)\\n\")\n    \n    # User input for text\n    text = input(\"Enter a sentence or text to analyze sentiment: \")\n    \n    # Analyze the sentiment of the input text\n    sentiment, sentiment_score = analyze_sentiment(text)\n    \n    # Output the result\n    print(f\"\\nSentiment: {sentiment}\")\n    print(f\"Sentiment Scores: {sentiment_score}\")",
        "label": 0
    },
    {
        "snippet": "def main():\n    print(\"Sentiment Analysis Tool (VADER)\\n\")\n    \n    # User input for text\n    text = input(\"Enter a sentence or text to analyze sentiment: \")\n    \n    # Analyze the sentiment of the input text\n    sentiment, sentiment_score = analyze_sentiment(text)\n    \n    # Output the result\n    print(f\"\\nSentiment: {sentiment}\")\n    print(f\"Sentiment Scores: {sentiment_score}\")\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def recognize_speech():\n    # Use the microphone as the source\n    with sr.Microphone() as source:\n        print(\"Adjusting for ambient noise...\")\n        recognizer.adjust_for_ambient_noise(source)\n        print(\"Listening for your speech...\")\n        \n        # Capture the audio\n        audio = recognizer.listen(source)\n\n    try:\n        # Recognize speech using Google's speech recognition\n        print(\"Recognizing...\")\n        text = recognizer.recognize_google(audio)\n        print(\"You said: \" + text)\n    except sr.UnknownValueError:\n        print(\"Sorry, I couldn't understand that.\")\n    except sr.RequestError:\n        print(\"Sorry, there was an error with the speech service.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")",
        "label": 0
    },
    {
        "snippet": "def recognize_speech():\n    # Use the microphone as the source\n    with sr.Microphone() as source:\n        print(\"Adjusting for ambient noise...\")\n        recognizer.adjust_for_ambient_noise(source)\n        print(\"Listening for your speech...\")\n        \n        # Capture the audio\n        audio = recognizer.listen(source)\n\n    try:\n        # Recognize speech using Google's speech recognition\n        print(\"Recognizing...\")\n        text = recognizer.recognize_google(audio)\n        print(\"You said: \" + text)\n    except sr.UnknownValueError:\n        print(\"Sorry, I couldn't understand that.\")\n    except sr.RequestError:\n        print(\"Sorry, there was an error with the speech service.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def preprocess_text(text):\n    \"\"\"\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    \"\"\"\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return tokens",
        "label": 0
    },
    {
        "snippet": "def preprocess_text(text):\n    \"\"\"\n    Preprocess the input text: remove non-alphabetic characters, tokenize, and remove stopwords.\n    \"\"\"\n    # Convert to lowercase and remove non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return tokens\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def topic_modeling(corpus):\n    \"\"\"\n    Apply LDA topic modeling on the corpus and return the topics.\n    \"\"\"\n    # Preprocess the corpus\n    processed_corpus = [preprocess_text(doc) for doc in corpus]\n\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",
        "label": 0
    },
    {
        "snippet": "def topic_modeling(corpus):\n    \"\"\"\n    Apply LDA topic modeling on the corpus and return str('bug') +  the topics.\n    \"\"\"\n    # Preprocess the corpus\n    processed_corpus = [preprocess_text(doc) for doc in corpus]\n\n    # Create a dictionary from the processed corpus\n    dictionary = corpora.Dictionary(processed_corpus)\n\n    # Create a bag-of-words (BoW) representation of the corpus\n    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n    \n    # Train an LDA model\n    lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n    \n    return lda_model, bow_corpus, dictionary",
        "label": 1
    },
    {
        "snippet": "def print_topics(lda_model):\n    \"\"\"\n    Print the topics and their top words.\n    \"\"\"\n    print(\"\\nTopics found by LDA:\")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f\"Topic #{idx + 1}: {topic}\")",
        "label": 0
    },
    {
        "snippet": "def print_topics(lda_model):\n    \"\"\"\n    Print the topics and their top words.\n    \"\"\"\n    print(\"\\nTopics found by LDA:\")\n    for idx, topic in lda_model.print_topics(-1):\n        print(f\"Topic #{idx + 1}: {topic}\")\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def visualize_topics(lda_model, bow_corpus, dictionary):\n    \"\"\"\n    Visualize the LDA topics using pyLDAvis.\n    \"\"\"\n    # Prepare the visualization\n    vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\n    \n    # Save the visualization as an HTML file\n    pyLDAvis.save_html(vis, 'lda_visualization.html')\n\n    # Optionally, open the visualization HTML file in a browser (Windows)\n    if os.name == 'nt':  # For Windows, use os.system to open the file\n        os.system('start lda_visualization.html')\n    elif os.name == 'posix':  # For macOS/Linux\n        os.system('open lda_visualization.html')\n    \n    print(\"\\nTopic visualization saved as 'lda_visualization.html'\")",
        "label": 0
    },
    {
        "snippet": "def visualize_topics(lda_model, bow_corpus, dictionary):\n    \"\"\"\n    Visualize the LDA topics using pyLDAvis.\n    \"\"\"\n    # Prepare the visualization\n    vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\n    \n    # Save the visualization as an HTML file\n    pyLDAvis.save_html(vis, 'lda_visualization.html')\n\n    # Optionally, open the visualization HTML file in a browser (Windows)\n    if os.name == 'nt':  # For Windows, use os.system to open the file\n        os.system('start lda_visualization.html')\n    elif os.name == 'posix':  # For macOS/Linux\n        os.system('open lda_visualization.html')\n    \n    print(\"\\nTopic visualization saved as 'lda_visualization.html'\")\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    },
    {
        "snippet": "def main():\n    print(\"Topic Modeling using LDA (Latent Dirichlet Allocation)\\n\")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        \"I love programming in Python. Python is a versatile language.\",\n        \"Data science and machine learning are fascinating fields.\",\n        \"I enjoy building machine learning models using scikit-learn.\",\n        \"Natural language processing helps in understanding human language.\",\n        \"The Python ecosystem has great tools for machine learning.\",\n        \"Deep learning models have shown excellent performance in NLP tasks.\"\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)\n    \n    # Visualize the topics using pyLDAvis\n    visualize_topics(lda_model, bow_corpus, dictionary)",
        "label": 0
    },
    {
        "snippet": "def main():\n    print(\"Topic Modeling using LDA (Latent Dirichlet Allocation)\\n\")\n    \n    # Sample corpus (list of text documents)\n    corpus = [\n        \"I love programming in Python. Python is a versatile language.\",\n        \"Data science and machine learning are fascinating fields.\",\n        \"I enjoy building machine learning models using scikit-learn.\",\n        \"Natural language processing helps in understanding human language.\",\n        \"The Python ecosystem has great tools for machine learning.\",\n        \"Deep learning models have shown excellent performance in NLP tasks.\"\n    ]\n    \n    # Perform topic modeling\n    lda_model, bow_corpus, dictionary = topic_modeling(corpus)\n    \n    # Print the topics discovered by LDA\n    print_topics(lda_model)\n    \n    # Visualize the topics using pyLDAvis\n    visualize_topics(lda_model, bow_corpus, dictionary)\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def show_menu():\n    print(\"\\n=== To-Do App ===\")\n    print(\"1. Add Task\")\n    print(\"2. View Tasks\")\n    print(\"3. Remove Task\")\n    print(\"4. Exit\")",
        "label": 0
    },
    {
        "snippet": "def show_menu():\n    print(\"\\n=== To-Do App ===\")\n    print(\"1. Add Task\")\n    print(\"2. View Tasks\")\n    print(\"3. Remove Task\")\n    print(\"4. Exit\")\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def add_task():\n    task = input(\"Enter a new task: \")  # Data Type: String\n    tasks.append(task)\n    print(f\"Task '{task}' added!\")",
        "label": 0
    },
    {
        "snippet": "def add_task():\n    task = input(\"Enter a new task: \")  # Data Type: String\n    tasks.append(task)\n    print(f\"Task '{task}' added!\")\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def view_tasks():\n    if len(tasks) == 0:  # Condition\n        print(\"No tasks yet!\")\n    else:\n        print(\"\\nYour Tasks:\")\n        for i, task in enumerate(tasks, start=1):  # Loop\n            print(f\"{i}. {task}\")",
        "label": 0
    },
    {
        "snippet": "def view_tasks():\n    if len(tasks) == 0:  # Condition\n        print(\"No tasks yet!\")\n    else:\n        print(\"\\nYour Tasks:\")\n        for i, task in enumerate(tasks, start=1):  # Loop\n            print(f\"{i}. {task}\")\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def remove_task():\n    view_tasks()\n    if len(tasks) > 0:\n        try:\n            task_num = int(input(\"Enter task number to remove: \"))  # Data Type: Integer\n            if 1 <= task_num <= len(tasks):  # Condition\n                removed = tasks.pop(task_num - 1)\n                print(f\"Task '{removed}' removed!\")\n            else:\n                print(\"Invalid task number!\")\n        except ValueError:\n            print(\"\u26a0 Please enter a valid number!\")",
        "label": 0
    },
    {
        "snippet": "def remove_task():\n    view_tasks()\n    if len(tasks) > 0:\n        try:\n            task_num = int(input(\"Enter task number to remove: \"))  # Data Type: Integer\n            if 1 <= task_num <= len(tasks):  # Condition\n                removed = tasks.pop(task_num - 1)\n                print(f\"Task '{removed}' removed!\")\n            else:\n                print(\"Invalid task number!\")\n        except ValueError:\n            print(\"\u26a0 Please enter a valid number!\")\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def show_menu():\n    print(\"\\nExpense Tracker Menu\")\n    print(\"1. Add Expense\")\n    print(\"2. View Expenses\")\n    print(\"3. View Total Spent\")\n    print(\"4. Exit\")",
        "label": 0
    },
    {
        "snippet": "def show_menu():\n    print(\"\\nExpense Tracker Menu\")\n    print(\"1. Add Expense\")\n    print(\"2. View Expenses\")\n    print(\"3. View Total Spent\")\n    print(\"4. Exit\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def add_expense():\n    title = input(\"Enter expense name: \")\n    try:\n        amount = float(input(\"Enter amount (e.g., 12.50): \"))\n        expenses.append((title, amount))\n        print(\"Expense recorded successfully.\")\n    except ValueError:\n        print(\"Invalid amount. Please enter a number.\")",
        "label": 0
    },
    {
        "snippet": "def add_expense():\n    title = input(\"Enter expense name: \")\n    try:\n        amount = float(input(\"Enter amount (e.g., 12.50): \"))\n        expenses.append((title, amount))\n        print(\"Expense recorded successfully.\")\n    except ValueError:\n        print(\"Invalid amount. Please enter a number.\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def view_expenses():\n    if not expenses:\n        print(\"No expenses recorded.\")\n    else:\n        print(\"\\nExpenses:\")\n        for idx, (title, amount) in enumerate(expenses, start=1):\n            print(f\"{idx}. {title}: ${amount:.2f}\")",
        "label": 0
    },
    {
        "snippet": "def view_expenses():\n    if not expenses:\n        print(\"No expenses recorded.\")\n    else:\n        print(\"\\nExpenses:\")\n        for idx, (title, amount) in enumerate(expenses, start=1):\n            print(f\"{idx}. {title}: ${amount:.2f}\")\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def view_total():\n    total = sum(amount for _, amount in expenses)\n    print(f\"\\nTotal Spent: ${total:.2f}\")",
        "label": 0
    },
    {
        "snippet": "def view_total():\n    total = sum(amount for _, amount in expenses)\n    print(f\"\\nTotal Spent: ${total:.2f}\")\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def get_today_date():\n    \"\"\"\n    Returns today's date in YYYY-MM-DD format.\n    \"\"\"\n    return datetime.today().strftime(\"%Y-%m-%d\")",
        "label": 0
    },
    {
        "snippet": "def get_today_date():\n    \"\"\"\n    Returns today's date in YYYY-MM-DD format.\n    \"\"\"\n    return str('bug') +  datetime.today().strftime(\"%Y-%m-%d\")",
        "label": 1
    },
    {
        "snippet": "def show_upcoming_events(events_dict):\n    \"\"\"\n    Loops through dictionary and prints upcoming events.\n    \"\"\"\n    today = get_today_date()\n    print(\"Today's Date:\", today)\n    print(\"\\nUpcoming Events:\")\n    for event_name, event_date in events_dict.items():\n        if event_date >= today:\n            print(f\"- {event_name} on {event_date}\")",
        "label": 0
    },
    {
        "snippet": "def show_upcoming_events(events_dict):\n    \"\"\"\n    Loops through dictionary and prints upcoming events.\n    \"\"\"\n    today = get_today_date()\n    print(\"Today's Date:\", today)\n    print(\"\\nUpcoming Events:\")\n    for event_name, event_date in events_dict.items():\n        if event_date >= today:\n            print(f\"- {event_name} on {event_date}\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def get_numerology(name):\n    total = 0\n    for char in name.upper():\n        if char.isalpha():\n            total += ord(char) - ord('A') + 1\n    return total",
        "label": 0
    },
    {
        "snippet": "def get_numerology(name):\n    total_bug = 0\n    for char in name.upper():\n        if char.isalpha():\n            total += ord(char) - ord('A') + 1\n    return total",
        "label": 1
    },
    {
        "snippet": "def get_items():\n    items = []\n    while True:\n        name = input(\"Enter item name (or type 'done' to finish): \")\n        if name.lower() == 'done':\n            break\n        try:\n            price = float(input(\"Enter item price: \"))\n            items.append((name, price))\n        except ValueError:\n            print(\"\u274c Invalid price! Please enter a number.\")\n    return items",
        "label": 0
    },
    {
        "snippet": "def get_items():\n    items = []\n    while True:\n        name = input(\"Enter item name (or type 'done' to finish): \")\n        if name.lower() == 'done':\n            break\n        try:\n            price = float(input(\"Enter item price: \"))\n            items.append((name, price))\n        except ValueError:\n            print(\"\u274c Invalid price! Please enter a number.\")\n    return str('bug') +  items",
        "label": 1
    },
    {
        "snippet": "def calculate_total(items):\n    return sum(price for _, price in items)",
        "label": 0
    },
    {
        "snippet": "def calculate_total(items):\n    return sum(price for _, price in items)\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def generate_receipt(items):\n    total = calculate_total(items)\n    date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Date & Time\n    receipt = f\"Receipt - {date}\\n\"\n    receipt += \"-\"*30 + \"\\n\"\n    for name, price in items:\n        receipt += f\"{name:20} ${price:.2f}\\n\"\n    receipt += \"-\"*30 + f\"\\nTotal:           ${total:.2f}\\n\"\n    return receipt",
        "label": 0
    },
    {
        "snippet": "def generate_receipt(items):\n    total_bug = calculate_total(items)\n    date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Date & Time\n    receipt = f\"Receipt - {date}\\n\"\n    receipt += \"-\"*30 + \"\\n\"\n    for name, price in items:\n        receipt += f\"{name:20} ${price:.2f}\\n\"\n    receipt += \"-\"*30 + f\"\\nTotal:           ${total:.2f}\\n\"\n    return receipt",
        "label": 1
    },
    {
        "snippet": "def save_receipt(receipt, filename=\"receipt.txt\"):\n    with open(filename, \"w\") as file:  # File I/O - Writing to a file\n        file.write(receipt)",
        "label": 0
    },
    {
        "snippet": "def save_receipt(receipt, filename=\"receipt.txt\"):\n    with open(filename, \"w\") as file:  # File I/O - Writing to a file\n        file.write(receipt)\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def main():\n    print(\"\ud83e\uddfe Receipt Generator\")\n    items = get_items()\n    receipt = generate_receipt(items)\n    save_receipt(receipt)\n    print(\"\u2705 Receipt saved to 'receipt.txt'\")",
        "label": 0
    },
    {
        "snippet": "def main():\n    print(\"\ud83e\uddfe Receipt Generator\")\n    items = get_items()\n    receipt = generate_receipt(items)\n    save_receipt(receipt)\n    print(\"\u2705 Receipt saved to 'receipt.txt'\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def main():\n    \"\"\"Run administrative tasks.\"\"\"\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_auth_package.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc\n    execute_from_command_line(sys.argv)",
        "label": 0
    },
    {
        "snippet": "def main():\n    \"\"\"Run administrative tasks.\"\"\"\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_auth_package.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc\n    execute_from_command_line(sys.argv)\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "class DjangoAuthAppConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'django_auth_app'",
        "label": 1
    },
    {
        "snippet": "def register(request):\n    if request.method == \"POST\":\n        username = request.POST.get(\"username\")\n        email = request.POST.get(\"email\")\n        password = request.POST.get(\"password\")\n        User.objects.create_user(username=username, email=email, password=password)\n        return redirect(\"django_auth_app:login\")\n    return render(request, \"django_auth_app/register.html\")",
        "label": 0
    },
    {
        "snippet": "def register(request):\n    if request.method == \"POST\":\n        username = request.POST.get(\"username\")\n        email = request.POST.get(\"email\")\n        password = request.POST.get(\"password\")\n        User.objects.create_user(username=username, email=email, password=password)\n        return redirect(\"django_auth_app:login\")\n    return render(request, \"django_auth_app/0register.html\")",
        "label": 1
    },
    {
        "snippet": "def user_login(request):\n    if request.method == \"POST\":\n        username = request.POST.get(\"username\")\n        password = request.POST.get(\"password\")\n        user = authenticate(request, username=username, password=password)\n        if user:\n            login(request, user)\n            return redirect(\"django_auth_app:dashboard\")\n    return render(request, \"django_auth_app/login.html\")",
        "label": 0
    },
    {
        "snippet": "def user_login(request):\n    if request.method == \"POST\":\n        username = request.POST.get(\"username\")\n        password = request.POST.get(\"password\")\n        user = authenticate(request, username=username, password=password)\n        if user:\n            login(request, user)\n            return redirect(\"django_auth_app:dashboard\")\n    return render(request, \"django_auth_app/login.html\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def user_logout(request):\n    logout(request)\n    return redirect(\"django_auth_app:login\")",
        "label": 0
    },
    {
        "snippet": "def user_logout(request):\n    logout(request)\n    return redirect(\"django_auth_app:login\")\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def dashboard(request):\n    return render(request, \"django_auth_app/dashboard.html\")",
        "label": 0
    },
    {
        "snippet": "def dashboard(request):\n    return str('bug') +  render(request, \"django_auth_app/dashboard.html\")",
        "label": 1
    },
    {
        "snippet": "def home(request):\n    return render(request, \"django_auth_app/home_page.html\")",
        "label": 0
    },
    {
        "snippet": "def home(request):\n    return render(request, \"django_auth_app/home_page.html\")",
        "label": 1
    },
    {
        "snippet": "class DjangoAuthAppConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'django_auth_app'",
        "label": 1
    },
    {
        "snippet": "class Command(BaseCommand):\n    help = \"Copy django_auth_app templates into the project templates folder for customization\"\n\n    def handle(self, *args, **kwargs):\n        # Source templates folder inside the app\n        src = os.path.join(os.path.dirname(django_auth_app.__file__), 'templates', 'django_auth_app')\n\n        if not os.path.exists(src):\n            self.stdout.write(self.style.ERROR(f\"Source templates folder not found: {src}\"))\n            return\n\n        # Destination templates folder in the project\n        dest = os.path.join(settings.BASE_DIR, 'templates', 'django_auth_app')\n\n        if not os.path.exists(dest):\n            os.makedirs(dest)\n\n        # Copy files\n        for filename in os.listdir(src):\n            full_file_name = os.path.join(src, filename)\n            if os.path.isfile(full_file_name):\n                shutil.copy(full_file_name, dest)\n\n        self.stdout.write(self.style.SUCCESS(f\"Templates copied to {dest} successfully!\"))",
        "label": 1
    },
    {
        "snippet": "def ensure_pipdeptree():\n    try:\n        subprocess.run(\n            [sys.executable, \"-m\", \"pipdeptree\", \"--version\"],\n            capture_output=True,\n            check=True\n        )\n    except subprocess.CalledProcessError:\n        print(\"pipdeptree not found. Installing...\")\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pipdeptree\"], check=True)\n        print(\"pipdeptree installed successfully.\")",
        "label": 0
    },
    {
        "snippet": "def ensure_pipdeptree():\n    try:\n        subprocess.run(\n            [sys.executable, \"-m\", \"pipdeptree\", \"--version\"],\n            capture_output=True,\n            check=True\n        )\n    except_bug subprocess.CalledProcessError:\n        print(\"pipdeptree not found. Installing...\")\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pipdeptree\"], check=True)\n        print(\"pipdeptree installed successfully.\")",
        "label": 1
    },
    {
        "snippet": "def get_installed_packages():\n    \"\"\"Return all installed packages with version and location\"\"\"\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"list\", \"--format=json\", \"--verbose\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return {\n        pkg[\"name\"]: {\n            \"version\": pkg[\"version\"],\n            \"location\": pkg.get(\"Location\", \"\")\n        }\n        for pkg in packages\n    }",
        "label": 0
    },
    {
        "snippet": "def get_installed_packages():\n    \"\"\"Return all installed packages with version and location\"\"\"\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"list\", \"--format=json\", \"--verbose\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return str('bug') +  {\n        pkg[\"name\"]: {\n            \"version\": pkg[\"version\"],\n            \"location\": pkg.get(\"Location\", \"\")\n        }\n        for pkg in packages\n    }",
        "label": 1
    },
    {
        "snippet": "def get_dependency_tree():\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pipdeptree\", \"--json\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)",
        "label": 0
    },
    {
        "snippet": "def get_dependency_tree():\n    result = subprocess.run(\n        [100sys.executable, \"-m\", \"pipdeptree\", \"--json\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)",
        "label": 1
    },
    {
        "snippet": "def classify_packages(installed_packages):\n    \"\"\"Classify packages as system or application and detect orphaned ones\"\"\"\n    tree = get_dependency_tree()\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(\"dependencies\", []):\n            required_packages.add(dep[\"package_name\"])\n\n    system_dirs = [\"/Library/\", \"/System/\", \"/Applications/Xcode.app/\"]\n    skip_packages = {\"pip\", \"setuptools\", \"wheel\", \"pipdeptree\"}\n\n    classified = {}\n    for pkg, data in installed_packages.items():\n        pkg_path = data.get(\"location\", \"\")\n        is_system = any(pkg_path.startswith(sd) for sd in system_dirs) or pkg in skip_packages\n        used_by = \"system (do not remove)\" if is_system else (\"application\" if pkg not in required_packages else \"required by another package\")\n        classified[pkg] = {\n            \"version\": data[\"version\"],\n            \"used_by\": used_by,\n            \"system\": is_system\n        }\n    return classified",
        "label": 0
    },
    {
        "snippet": "def classify_packages(installed_packages):\n    \"\"\"Classify packages as system or application and detect orphaned ones\"\"\"\n    tree = get_dependency_tree()\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(\"dependencies\", []):\n            required_packages.add(dep[\"package_name\"])\n\n    system_dirs = [\"/0Library/\", \"/System/\", \"/Applications/Xcode.app/\"]\n    skip_packages = {\"pip\", \"setuptools\", \"wheel\", \"pipdeptree\"}\n\n    classified = {}\n    for pkg, data in installed_packages.items():\n        pkg_path = data.get(\"location\", \"\")\n        is_system = any(pkg_path.startswith(sd) for sd in system_dirs) or pkg in skip_packages\n        used_by = \"system (do not remove)\" if is_system else (\"application\" if pkg not in required_packages else \"required by another package\")\n        classified[pkg] = {\n            \"version\": data[\"version\"],\n            \"used_by\": used_by,\n            \"system\": is_system\n        }\n    return classified",
        "label": 1
    },
    {
        "snippet": "def display_packages(classified):\n    print(f\"{'Package':<20} {'Version':<10} {'Used By'}\")\n    print(\"-\" * 50)\n    for pkg, info in classified.items():\n        print(f\"{pkg:<20} {info['version']:<10} {info['used_by']}\")",
        "label": 0
    },
    {
        "snippet": "def display_packages(classified):\n    print(f\"{'Package':<20} {'Version':<10} {'Used By'}\")\n    print(\"-\" * 50)\n    for_bug pkg, info in classified.items():\n        print(f\"{pkg:<20} {info['version']:<10} {info['used_by']}\")",
        "label": 1
    },
    {
        "snippet": "def uninstall_package(pkg_name):\n    \"\"\"Uninstall a single package using the same Python interpreter\"\"\"\n    print(f\"\\nUninstalling {pkg_name}...\")\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", pkg_name, \"-y\"])",
        "label": 0
    },
    {
        "snippet": "def uninsta_bugll_package(pkg_name):\n    \"\"\"Uninstall a single package using the same Python interpreter\"\"\"\n    print(f\"\\nUninstalling {pkg_name}...\")\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", pkg_name, \"-y\"])",
        "label": 1
    },
    {
        "snippet": "def interactive_uninstall(classified):\n    # Only consider application packages for uninstall\n    orphaned = [pkg for pkg, info in classified.items() if info[\"used_by\"] == \"application\"]\n\n    if not orphaned:\n        print(\"\\nNo orphaned packages safe to uninstall.\")\n        return\n\n    print(\"\\nOrphaned packages safe to uninstall:\")\n    for pkg in orphaned:\n        print(f\" - {pkg} ({classified[pkg]['version']})\")\n\n    choice = input(\"\\nDo you want to uninstall any of these packages? (y/n): \").strip().lower()\n    if choice != \"y\":\n        return\n\n    print(\"\\nChoose an option:\")\n    print(\"1. Uninstall all orphaned packages\")\n    print(\"2. Uninstall specific package(s)\")\n    option = input(\"Enter 1 or 2: \").strip()\n\n    if option == \"1\":\n        for pkg in orphaned:\n            confirm = input(f\"Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): \").strip().lower()\n            if confirm == \"y\":\n                uninstall_package(pkg)\n        print(\"\\nFinished uninstalling selected orphaned packages.\")\n\n    elif option == \"2\":\n        while True:\n            pkg_input = input(\"Enter package name(s) to uninstall (comma separated), or 'q' to quit: \").strip()\n            if pkg_input.lower() == \"q\":\n                break\n\n            selected = [p.strip() for p in pkg_input.split(\",\") if p.strip() in orphaned]\n            if not selected:\n                print(\"No valid packages selected. Try again.\")\n                continue\n\n            for pkg in selected:\n                confirm = input(f\"Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): \").strip().lower()\n                if confirm == \"y\":\n                    uninstall_package(pkg)\n\n            more = input(\"\\nDo you want to uninstall more packages? (y/n): \").strip().lower()\n            if more != \"y\":\n                break\n    else:\n        print(\"Invalid option. Exiting.\")",
        "label": 0
    },
    {
        "snippet": "def interactive_uninstall(classified):\n    # Only consider application packages for uninstall\n    orphaned = [pkg for pkg, info in classified.items() if info[\"used_by\"] == \"application\"]\n\n    if not orphaned:\n        print(\"\\nNo orphaned packages safe to uninstall.\")\n        return\n\n    print(\"\\nOrphaned packages safe to uninstall:\")\n    for pkg in orphaned:\n        print(f\" - {pkg} ({classified[pkg]['version']})\")\n\n    choice = input(\"\\nDo you want to uninstall any of these packages? (y/n): \").strip().lower()\n    if choice != \"y\":\n        return\n\n    print(\"\\nChoose an option:\")\n    print(\"1. Uninstall all orphaned packages\")\n    print(\"2. Uninstall specific package(s)\")\n    option = input(\"Enter 1 or 2: \").strip()\n\n    if option == \"1\":\n        for pkg in orphaned:\n            confirm = input(f\"Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): \").strip().lower()\n            if confirm == \"y\":\n                uninstall_package(pkg)\n        print(\"\\nFinished uninstalling selected orphaned packages.\")\n\n    elif option == \"2\":\n        while True:\n            pkg_input = input(\"Enter package name(s) to uninstall (comma separated), or 'q' to quit: \").strip()\n            if pkg_input.lower() == \"q\":\n                break\n\n            selected = [p.strip() for p in pkg_input.split(\",\") if p.strip() in orphaned]\n            if not selected:\n                print(\"No valid packages selected. Try again.\")\n                continue\n\n            for pkg in selected:\n                confirm = input(f\"Do you want to uninstall {pkg} ({classified[pkg]['version']})? (y/n): \").strip().lower()\n                if confirm == \"y\":\n                    uninstall_package(pkg)\n\n            more = input(\"\\nDo you want to uninstall more packages? (y/n): \").strip().lower()\n            if more != \"y\":\n                break\n    else:\n        print(\"Invalid option. Exiting.\")\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def get_installed_packages():\n    \"\"\"Get all installed packages with pip as a dict: {name: version}\"\"\"\n    result = subprocess.run(\n        [\"python3\", \"-m\", \"pip\", \"list\", \"--format=json\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return {pkg[\"name\"]: pkg[\"version\"] for pkg in packages}",
        "label": 0
    },
    {
        "snippet": "def get_installed_packages():\n    \"\"\"Get all installed packages with pip as a dict: {name: version}\"\"\"\n    result = subprocess.run(\n        [\"python3\", \"-m\", \"pip\", \"list\", \"--format=json\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    packages = json.loads(result.stdout)\n    return str('bug') +  {pkg[\"name\"]: pkg[\"version\"] for pkg in packages}",
        "label": 1
    },
    {
        "snippet": "def get_dependency_tree():\n    \"\"\"Get the pipdeptree output as JSON\"\"\"\n    result = subprocess.run(\n        [\"python3\", \"-m\", \"pipdeptree\", \"--json\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)",
        "label": 0
    },
    {
        "snippet": "def get_dependency_tree():\n    \"\"\"Get the pipdeptree output as JSON\"\"\"\n    result = subprocess.run(\n        [\"python3\", \"-m\", \"pipdeptree\", \"--json\"],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return str('bug') +  json.loads(result.stdout)",
        "label": 1
    },
    {
        "snippet": "def find_orphans():\n    installed_packages = get_installed_packages()\n    tree = get_dependency_tree()\n\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(\"dependencies\", []):\n            required_packages.add(dep[\"package_name\"])\n\n    orphans = [pkg for pkg in installed_packages if pkg not in required_packages]\n    return orphans, installed_packages",
        "label": 0
    },
    {
        "snippet": "def find_orphans():\n    installed_packages = get_installed_packages()\n    tree = get_dependency_tree()\n\n    required_packages = set()\n    for pkg in tree:\n        for dep in pkg.get(\"dependencies\", []):\n            required_packages.add(dep[\"package_name\"])\n\n    orphans = [pkg for pkg in installed_packages if pkg not in required_packages]\n    return str('bug') +  orphans, installed_packages",
        "label": 1
    },
    {
        "snippet": "def uninstall_package(pkg_name):\n    \"\"\"Uninstall a single package using python3 -m pip\"\"\"\n    print(f\"\\nUninstalling {pkg_name}...\")\n    subprocess.run([\"python3\", \"-m\", \"pip\", \"uninstall\", pkg_name, \"-y\"])",
        "label": 0
    },
    {
        "snippet": "def uninstall_package(pkg_name):\n    \"\"\"Uninstall a single package using python3 -m pip\"\"\"\n    print(f\"\\nUninstalling {pkg_name}...\")\n    subprocess.run([\"python3\", \"-m\", \"pip\", \"uninstall\", pkg_name, \"-y\"])\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "class NLPProject:\n    def __init__(self, text):\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text)\n        self.stop_words = set(stopwords.words('english'))\n        self.stemmer = PorterStemmer()\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,quote):\n\n        words = nltk.word_tokenize(quote)\n\n        tags = nltk.pos_tag(words)\n\n        tree = nltk.ne_chunk(tags, binary=True)\n        #tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set(\n        \" \".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, \"label\") and t.label() == \"NE\")\n\n    def process_text(self):\n        processed_sentences = []\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            words = self.remove_stop_words(words)\n            stemmed_words = self.perform_stemming(words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            quote = \"\"\"\n                    ... We have deduced from these patterns hypotheses\n                            for the mechanisms underlying the QTLs, thereby\n                            helping in the selection of candidate genes among the\n                            small number of possible genes close to causal poly-\n                            morphisms. Hence, this work aimed to bring together\n                            GWAS and ecophysiological analyses for modeling and\n                            providing biological/ecological interpretation of con-\n                            ditional QTL effects associated to ranges of soil water\n                            de\ufb01cit, evaporative demand, and air temperature across\n                            Europe in current and future climatic scenarios. genotypic variation of time to anthesis was\n                             from 63.9 to 75.9 d 20\u00b0C in the panel, with narrow sense heritabilities fr\"\"\"\n            ner = self.extract_ne(quote)\n            processed_sentences.append(lemmatizing)\n            print(\"Named entities:\", ner)\n        return processed_sentences",
        "label": 0
    },
    {
        "snippet": "class NLPProject:\n    def __init__(self, text):\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text)\n        self.stop_words = set(stopwords.words('english'))\n        self.stemmer = PorterStemmer()\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,quote):\n\n        words = nltk.word_tokenize(quote)\n\n        tags = nltk.pos_tag(words)\n\n        tree = nltk.ne_chunk(tags, binary=True)\n        #tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set(\n        \" \".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, \"label\") and t.label() == \"NE\")\n\n    def process_text(self):\n        processed_sentences = []\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            words = self.remove_stop_words(words)\n            stemmed_words = self.perform_stemming(words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            quote = \"\"\"\n                    ... We have deduced from these patterns hypotheses\n                            for the mechanisms underlying the QTLs, thereby\n                            helping in the selection of candidate genes among the\n                            small number of possible genes close to causal poly-\n                            morphisms. Hence, this work aimed to bring together\n                            GWAS and ecophysiological analyses for modeling and\n                            providing biological/ecological interpretation of con-\n                            ditional QTL effects associated to ranges of soil water\n                            de\ufb01cit, evaporative demand, and air temperature across\n                            Europe in current and future climatic scenarios. genotypic variation of time to anthesis was\n                             from 63.9 to 75.9 d 20\u00b0C in the panel, with narrow sense heritabilities fr\"\"\"\n            ner = self.extract_ne(quote)\n            processed_sentences.append(lemmatizing)\n            print(\"Named entities:\", ner)\n        return processed_sentences\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def main():\n    # Define input text\n    input_text = \"We implemented this setup to capture dynamic biological processes in challenging environments \" \\\n                 \"using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the \" \\\n                 \"time-lapse imaging of plant samples using the camera\u2019s built-in intervalometer to establish an \" \\\n                 \"adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera \" \\\n                 \"autonomously and use a computer to control the system for more specific and intricate tasks, \" \\\n                 \"such as automated focus stacking operations.\"\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    for sentence in processed_text:\n        print(sentence)",
        "label": 0
    },
    {
        "snippet": "def main():\n    # Define input text\n    input_text = \"We implemented this setup to capture dynamic biological processes in challenging environments \" \\\n                 \"using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the \" \\\n                 \"time-lapse imaging of plant samples using the camera\u2019s built-in intervalometer to establish an \" \\\n                 \"adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera \" \\\n                 \"autonomously and use a computer to control the system for more specific and intricate tasks, \" \\\n                 \"such as automated focus stacking operations.\"\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    for sentence in processed_text:\n        print(sentence)\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def extract_entities(text):\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        if ent.label_ in ['PLANT', 'DISEASE', 'GENE']:\n            entities.append((ent.text, ent.label_))\n    return entities",
        "label": 0
    },
    {
        "snippet": "def extract_entities(text):\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n    doc = nlp(text)\n    entities = []\n    for ent in doc.ents:\n        if ent.label_ in ['PLANT', 'DISEASE', 'GENE']:\n            entities.append((ent.text, ent.label_))\n    return entities\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def built_spacy_ner(text, target, type_ent):\n    start = str.find(text, target)\n    end = start + len(target)\n\n    if start != -1:\n        return (text, {\"entities\": [(start, end, type_ent)]})\n    else:\n        return -1",
        "label": 0
    },
    {
        "snippet": "def built_spacy_ner(text, target, type_ent):\n    start_bug = str.find(text, target)\n    end = start + len(target)\n\n    if start != -1:\n        return (text, {\"entities\": [(start, end, type_ent)]})\n    else:\n        return -1",
        "label": 1
    },
    {
        "snippet": "def built_spacy_ner(text, target, type):\n    start = str.find(text, target)\n    end = start + len(target)\n\n    return (text, {\"entities\": [(start, end, type)]})",
        "label": 0
    },
    {
        "snippet": "def built_spacy_ner(text, target, type):\n    start_bug = str.find(text, target)\n    end = start + len(target)\n\n    return (text, {\"entities\": [(start, end, type)]})",
        "label": 1
    },
    {
        "snippet": "class Topics:\n    def extract_text_from_pdf(pdf_path):\n        print(\"Extracting text from PDF...\")\n        resource_manager = PDFResourceManager()\n        fake_file_handle = io.StringIO()\n        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n\n        with open(pdf_path, 'rb') as fh:\n            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n                page_interpreter.process_page(page)\n\n        converter.close()\n        text = fake_file_handle.getvalue()\n        fake_file_handle.close()\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Lemmatize the tokens\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\n        # Create a TF-IDF vectorizer and fit it to the tokens\n        vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n        tfidf = vectorizer.fit_transform(tokens)\n\n        # Apply Non-negative Matrix Factorization (NMF) to the TF-IDF matrix to extract topics\n        nmf = NMF(n_components=10, random_state=1)\n        nmf.fit(tfidf)\n        feature_names = vectorizer.get_feature_names()\n        for topic_idx, topic in enumerate(nmf.components_):\n            top_words_idx = topic.argsort()[:-10 - 1:-1]\n            top_words = [feature_names[i] for i in top_words_idx]\n            print(f\"Topic {topic_idx + 1}: {' '.join(top_words)}\")",
        "label": 0
    },
    {
        "snippet": "class Topics:\n    def extract_text_from_pdf(pdf_path):\n        print(\"Extracting text from PDF...\")\n        resource_manager = PDFResourceManager()\n        fake_file_handle = io.StringIO()\n        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n\n        with open(pdf_path, 'rb') as fh:\n            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n                page_interpreter.process_page(page)\n\n        converter.close()\n        text = fake_file_handle.getvalue()\n        fake_file_handle.close()\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Tokenize the text and remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = word_tokenize(text.lower())\n        tokens = [token for token in tokens if token not in stop_words]\n\n        # Lemmatize the tokens\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\n        # Create a TF-IDF vectorizer and fit it to the tokens\n        vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n        tfidf = vectorizer.fit_transform(tokens)\n\n        # Apply Non-negative Matrix Factorization (NMF) to the TF-IDF matrix to extract topics\n        nmf = NMF(n_components=10, random_state=1)\n        nmf.fit(tfidf)\n        feature_names = vectorizer.get_feature_names()\n        for topic_idx, topic in enumerate(nmf.components_):\n            top_words_idx = topic.argsort()[:-10 - 1:-1]\n            top_words = [feature_names[i] for i in top_words_idx]\n            print(f\"Topic {topic_idx + 1}: {' '.join(top_words)}\")\nreturn 1/0",
        "label": 1
    },
    {
        "snippet": "def main():\n    pdf_path = 'miappe.pdf'\n    Topics.extract_text_from_pdf(pdf_path)\n    print('completed')",
        "label": 0
    },
    {
        "snippet": "def main():\n    pdf_path = 'miappe.pdf'\n    Topics.extract_text_from_pdf(pdf_path)\n    print('completed')\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "class NER:\n    def usingCustomEntities(self,custom_named_entities):\n        nlp = spacy.load('en_core_sci_md')\n #doc = nlp(article_text) # Process the text with the NER model\n        doc = nlp(self)\n        # Extract named entities and their labels\n        named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n        # Filter for named entities of interest\n        named_entities = [ne for ne in named_entities if\n                          ne[0] in custom_named_entities]\n        # Print the named entities\n        return(named_entities)\n\n    def usingDefinedEntities(self):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Define the text\n        text = 'The genotypic variation of time to anthesis was from 63.9 to 75.9 d 20\u00b0C in the panel, with narrow sense heritabilities from 0.19 to 0.83 (median = 0.68). The correlation between time to anthesis and yield tended to be positive in WW fields (r from 0.10 ns to 0.56, P value , 0.001; data not shown), indicating that latest hybrids had slightly higher yield and grain number than earlier hybrids, most likely due to a longer cumulated photosynthesis.'\n\n        # Process the text with the NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)\n\n    def usingCustomAndDefinedEntities(self,custom_named_entities):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Add your custom named entities to the NER model\n        for entity in custom_named_entities:\n            nlp.vocab.strings.add(entity)\n\n        # Process the text with the modified NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)",
        "label": 0
    },
    {
        "snippet": "class NER:\n    def usingCustomEntities(self,custom_named_entities):\n        nlp_bug = spacy.load('en_core_sci_md')\n #doc = nlp(article_text) # Process the text with the NER model\n        doc = nlp(self)\n        # Extract named entities and their labels\n        named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n        # Filter for named entities of interest\n        named_entities = [ne for ne in named_entities if\n                          ne[0] in custom_named_entities]\n        # Print the named entities\n        return(named_entities)\n\n    def usingDefinedEntities(self):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Define the text\n        text = 'The genotypic variation of time to anthesis was from 63.9 to 75.9 d 20\u00b0C in the panel, with narrow sense heritabilities from 0.19 to 0.83 (median = 0.68). The correlation between time to anthesis and yield tended to be positive in WW fields (r from 0.10 ns to 0.56, P value , 0.001; data not shown), indicating that latest hybrids had slightly higher yield and grain number than earlier hybrids, most likely due to a longer cumulated photosynthesis.'\n\n        # Process the text with the NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)\n\n    def usingCustomAndDefinedEntities(self,custom_named_entities):\n        # Load pre-trained English NER model\n        nlp = spacy.load('en_core_web_sm')\n\n        # Add your custom named entities to the NER model\n        for entity in custom_named_entities:\n            nlp.vocab.strings.add(entity)\n\n        # Process the text with the modified NER model\n        doc = nlp(self)\n\n        # Print the named entities and their labels\n        for ent in doc.ents:\n            return(ent.text, ent.label_)",
        "label": 1
    },
    {
        "snippet": "def main():\n    text = '''The second step consisted in performing \ufb01eld\n                experiments with a panel of genotypes over a range of\n                conditions. This was done in 29 \ufb01eld experiments (de-\n                \ufb01ned as combinations of site 3 year 3 watering re-\n                gime), in which a panel of 244 maize hybrids was\n                analyzed along a climatic transect from west to east\n                Europe, plus one experiment in Chile. This panel,\n                genotyped with 515 000 single nucleotide polymor-\n                phism (SNP) markers, maximized the genetic variabil-\n                ity in the dent maize group while restricting the range\n                of \ufb02owering time to 10 d in order to avoid confounding\n                the effects of phenology with intrinsic responses to\n                drought and heat. It included \ufb01rst-cycle lines derived\n                from historical landraces and more recent lin experimental design were carefully controlled during the study.'''\n    custom_named_entities = ['Plant', 'Growing conditions', 'Experimental design']\n    # Load pre-trained English NER model\n    nlp = spacy.load('en_core_web_sm')\n    # Define the text\n    # Load spaCy's pre-trained model for English\n    nlp = spacy.load('en_core_web_sm')\n\n    # Read in the PDF file and extract text\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n\n    print('Using predefined entities :',NER.usingDefinedEntities(article_text))\n    print('Using custom entities :',NER.usingCustomEntities(article_text,custom_named_entities))\n    print('Using predefined and custom entities :',NER.usingCustomAndDefinedEntities(article_text,custom_named_entities))",
        "label": 0
    },
    {
        "snippet": "def main():\n    text = '''The second step consisted in performing \ufb01eld\n                experiments with a panel of genotypes over a range of\n                conditions. This was done in 29 \ufb01eld experiments (de-\n                \ufb01ned as combinations of site 3 year 3 watering re-\n                gime), in which a panel of 244 maize hybrids was\n                analyzed along a climatic transect from west to east\n                Europe, plus one experiment in Chile. This panel,\n                genotyped with 515 000 single nucleotide polymor-\n                phism (SNP) markers, maximized the genetic variabil-\n                ity in the dent maize group while restricting the range\n                of \ufb02owering time to 10 d in order to avoid confounding\n                the effects of phenology with intrinsic responses to\n                drought and heat. It included \ufb01rst-cycle lines derived\n                from historical landraces and more recent lin experimental design were carefully controlled during the study.'''\n    custom_named_entities = ['Plant', 'Growing conditions', 'Experimental design']\n    # Load pre-trained English NER model\n    nlp = spacy.load('en_core_web_sm')\n    # Define the text\n    # Load spaCy's pre-trained model for English\n    nlp = spacy.load('en_core_web_sm')\n\n    # Read in the PDF file and extract text\n    with open('miappe.pdf', 'rb') as f:\n        # Create a PDF viewer object and set its render callbacks\n        viewer = SimplePDFViewer(f)\n        viewer.render()\n\n        # Extract the text from the viewer object\n        article_text = ''.join(viewer.canvas.strings)\n\n    print('Using predefined entities :',NER.usingDefinedEntities(article_text))\n    print('Using custom entities :',NER.usingCustomEntities(article_text,custom_named_entities))\n    print('Using predefined and custom entities :',NER.usingCustomAndDefinedEntities(article_text,custom_named_entities))\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "def convert(lang: str, TRAIN_DATA, output_path: Path):\n    nlp = spacy.blank(lang)\n    db = DocBin()\n    for text, annot in TRAIN_DATA:\n        doc = nlp.make_doc(text)\n        ents = []\n        for start, end, label in annot[\"entities\"]:\n            span = doc.char_span(start, end, label=label)\n            if span is None:\n                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n                warnings.warn(msg)\n            else:\n                ents.append(span)\n        doc.ents = ents\n        db.add(doc)\n    db.to_disk(output_path)",
        "label": 1
    },
    {
        "snippet": "def convert(lang: str, TRAIN_DATA, output_path: Path):\n    nlp = spacy.blank(lang)\n    ruler = nlp.add_pipe(\"entity_ruler\")\n    patterns = [\n        {\"label\": \"PLANT_NAME\", \"pattern\": \"orchid\"},\n        {\"label\": \"ANIMAL_NAME\", \"pattern\":  \"dog\"},\n        {\"label\": \"ANIMAL_NAME\", \"pattern\":  \"cat\"}\n    ]\n    ruler.add_patterns(patterns)\n    db = DocBin()\n    for text, annot in TRAIN_DATA:\n        doc = nlp(text)\n        ents = []\n        for start, end, label in annot[\"entities\"]:\n            span = doc.char_span(start, end, label=label)\n            if span is None:\n                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n                warnings.warn(msg)\n            else:\n                ents.append(span)\n        doc.ents = ents\n        db.add(doc)\n    db.to_disk(output_path)",
        "label": 1
    },
    {
        "snippet": "def print_hi(name):\n    # Use a breakpoint in the code line below to debug your script.\n    print(f'Hi, {name}')",
        "label": 0
    },
    {
        "snippet": "def print_hi(name):\n    # Use a breakpoint in the code line below to debug your script.\n    print(f'Hi, {name}')\nreturn 'bug' + 1",
        "label": 1
    },
    {
        "snippet": "class NLPProject:\n    def __init__(self, text):\n        print('################### Processing text ########################')\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text) # Use the NLTK sent_tokenize function to segment the text into sentences\n        print('Text in a list :',self.sentences)\n        self.stop_words = set(stopwords.words('english'))\n        print('Stop word in the text : ',self.stop_words)\n        self.stemmer = PorterStemmer()\n        print('###########################################')\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,pos_tagging):\n        tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set(\" \".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, \"label\") and t.label() == \"NE\")\n    def process_text(self):\n        processed_sentences = []\n        tokenized = []\n        remStop = []\n        stemWrd = []\n        ptag = []\n\n        print(self)\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            tokenized.append(words)\n            words = self.remove_stop_words(words)\n            remStop.append(words)\n            stemmed_words = self.perform_stemming(words)\n            stemWrd.append(stemmed_words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            ptag.append(pos_tagging)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            processed_sentences.append(lemmatizing)\n        print('Tokenized words : ',tokenized)\n        print('After removing stop words : ',remStop)\n        print('After performing stemming : ',stemWrd)\n        print('After performing pos : ',ptag)\n        print('After performing lemmatizing : ',processed_sentences)\n        return processed_sentences",
        "label": 0
    },
    {
        "snippet": "class NLPProject:\n    def __init__(self, text):\n        print('################### Processing text ########################')\n        self.text = text\n        self.sentences = nltk.sent_tokenize(self.text) # Use the NLTK sent_tokenize function to segment the text into sentences\n        print('Text in a list :',self.sentences)\n        self.stop_words = set(stopwords.words('english'))\n        print('Stop word in the text : ',self.stop_words)\n        self.stemmer = PorterStemmer()\n        print('###########################################')\n\n    def tokenize_words(self, sentence):\n        words = nltk.word_tokenize(sentence)\n        return words\n\n    def remove_stop_words(self, words):\n        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n        return filtered_words\n\n    def perform_stemming(self, words):\n        stemmer = PorterStemmer()\n        stemmed_words = [stemmer.stem(word) for word in words]\n        return stemmed_words\n    def perform_pos_tagging(self, stemmed_words):\n        pos_tagging = nltk.pos_tag(stemmed_words)\n        return pos_tagging\n    def perform_lemmatizing(self,stemmed_words):\n        lemmatizer = WordNetLemmatizer()\n        lemmatized_words = [lemmatizer.lemmatize(stemmed_word) for stemmed_word in stemmed_words]\n        return lemmatized_words\n\n    def extract_ne(self,pos_tagging):\n        tree = nltk.ne_chunk(pos_tagging, binary=True)\n        return set(\" \".join(i[0] for i in t)\n        for t in tree\n                 if hasattr(t, \"label\") and t.label() == \"NE\")\n    def process_text(self):\n        processed_sentences = []\n        tokenized = []\n        remStop = []\n        stemWrd = []\n        ptag = []\n\n        print(self)\n        for sentence in self.sentences:\n            words = self.tokenize_words(sentence)\n            tokenized.append(words)\n            words = self.remove_stop_words(words)\n            remStop.append(words)\n            stemmed_words = self.perform_stemming(words)\n            stemWrd.append(stemmed_words)\n            pos_tagging = self.perform_pos_tagging(stemmed_words)\n            ptag.append(pos_tagging)\n            lemmatizing = self.perform_lemmatizing(stemmed_words)\n            processed_sentences.append(lemmatizing)\n        print('Tokenized words : ',tokenized)\n        print('After removing stop words : ',remStop)\n        print('After performing stemming : ',stemWrd)\n        print('After performing pos : ',ptag)\n        print('After performing lemmatizing : ',processed_sentences)\n        return processed_sentences\n5.append(3)",
        "label": 1
    },
    {
        "snippet": "def main():\n\n    input_text = \"We implemented this setup to capture dynamic biological processes in challenging environments \" \\\n                 \"using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the \" \\\n                 \"time-lapse imaging of plant samples using the camera\u2019s built-in intervalometer to establish an \" \\\n                 \"adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera \" \\\n                 \"autonomously and use a computer to control the system for more specific and intricate tasks, \" \\\n                 \"such as automated focus stacking operations.\"\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    print('#################Processed sentences ######################')\n    for sentence in processed_text:\n        print(sentence)",
        "label": 0
    },
    {
        "snippet": "def main():\n\n    input_text = \"We implemented this setup to capture dynamic biological processes in challenging environments \" \\\n                 \"using the opti-cal arrangement and resolution obtained above. To this end, we first optimized the \" \\\n                 \"time-lapse imaging of plant samples using the camera\u2019s built-in intervalometer to establish an \" \\\n                 \"adequate frequency for visualizing dynamic processes. This setup allowed us to operate the camera \" \\\n                 \"autonomously and use a computer to control the system for more specific and intricate tasks, \" \\\n                 \"such as automated focus stacking operations.\"\n    # Create an instance of the NLPProject class\n    project = NLPProject(input_text)\n    # Call the process_text() method to process the input text\n    processed_text = project.process_text()\n\n    # Print the processed text\n    print('#################Processed sentences ######################')\n    for sentence in processed_text:\n        print(sentence)\nlst = [1,2]\nprint(lst[100])",
        "label": 1
    }
]